{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocessor(text:str, stemmer: str='Snowball', remove_mbti: bool=False) -> list:\n",
    "\t'''\n",
    "\tInput: str\n",
    "\tOutput: list\n",
    "\t\tPreprocessed tokens\n",
    "\tstemmer: str\n",
    "\t\tCan be 'Snowball' or 'Porter'. Default is Snowball.\n",
    "\tremove_mbti: bool\n",
    "\t\tRemove MBTI keywords like INTJ, ENFP, etc. Default is False.(Keep MBTI keywords.)\n",
    "\t'''\n",
    "\t# Cleaning\n",
    "\ttext = re.sub(r'\\|\\|\\|', ' ', text)  # Split by separator\n",
    "\ttext = re.sub(r'http\\S+', ' ', text)  # Replace hyperlink\n",
    "\ttext = re.sub(r\"[A-Za-z]+\\'+\\w+\", ' ', text)  # Handling apostrophe (e.g. you've, there's)\n",
    "\ttext = re.sub('[^0-9a-zA-Z]',' ', text)  # Keep only numbers and alphabets (remove special characters)\n",
    "\ttext = text.lower()\n",
    "\tif remove_mbti == True:\n",
    "\t\ttext = re.sub('intj|intp|entj|entp|infp|enfj|enfp|istj|isfj|estj|esfj|istp|isfp|estp|esfp|infj', '', text)\n",
    "  \t# Tokenization\n",
    "\ttokens = word_tokenize(text)\n",
    "\tfiltered_tokens = [w for w in tokens if not w in stopwords.words('english')]  # Remove stopwords\n",
    "\t# Stemming\n",
    "\tstemmer_ = SnowballStemmer(\"english\")\n",
    "\tif stemmer == 'Porter|porter':\n",
    "\t\tstemmer_ = PorterStemmer()\n",
    "\tif stemmer not in ['Snowball', 'snowball', 'Porter', 'porter']:\n",
    "\t\traise ValueError(\"Please check passed argument: stemmer must be 'Snowball' or 'Porter'\")\n",
    "\tstemmed = [stemmer_.stem(t) for t in filtered_tokens]\n",
    "\t# Lemmatizing\n",
    "\tlemma = WordNetLemmatizer()\n",
    "\t# lemmatized = [lemma.lemmatize(t) for t in stemmed]\n",
    "\tlemmatized = \" \".join([lemma.lemmatize(w) for w in stemmed])   # .join() -> 用空格分開每個字\n",
    "\treturn lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 : training data : snowball stemmer with removing MBTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CountVectorizer :\n",
      "\n",
      "Using Tf-idf :\n",
      "Now the dataset size is as below\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8675, 77959)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../data/Kaggle_MBTI.csv')\n",
    "data_snow_removeMBTI = data.copy()\n",
    "\n",
    "for d in range(len(data)):\n",
    "    post = data.loc[d, 'posts']\n",
    "\n",
    "    txt_snow_removenMBTI = Preprocessor(post, remove_mbti=True)\n",
    "\n",
    "    data_snow_removeMBTI.posts[d] = txt_snow_removenMBTI\n",
    "\n",
    "list_posts = []\n",
    "for i in range(len(data_snow_removeMBTI)):\n",
    "    post = data_snow_removeMBTI.loc[i, 'posts']\n",
    "    list_posts.append(post)\n",
    "list_posts = np.array(list_posts)\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Vectorizing the database posts to a matrix of token counts for the model\n",
    "cntizer = CountVectorizer() \n",
    "                        \n",
    "# the feature should be made of word n-gram \n",
    "# Learn the vocabulary dictionary and return term-document matrix\n",
    "print(\"Using CountVectorizer :\")\n",
    "X_cnt = cntizer.fit_transform(list_posts)\n",
    "col_name = cntizer.get_feature_names_out()   # 紀錄 sparse matrix 的字分別是哪些字\n",
    "\n",
    "\n",
    "# For the Standardization or Feature Scaling Stage :-\n",
    "# Transform the count matrix to a normalized tf or tf-idf representation\n",
    "tfizer = TfidfTransformer()\n",
    "\n",
    "# Learn the idf vector (fit) and transform a count matrix to a tf-idf representation\n",
    "print(\"\\nUsing Tf-idf :\")\n",
    "\n",
    "print(\"Now the dataset size is as below\")\n",
    "X_tfidf =  tfizer.fit_transform(X_cnt).toarray()\n",
    "X_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INFP    1832\n",
       "INFJ    1470\n",
       "INTP    1304\n",
       "INTJ    1091\n",
       "ENTP     685\n",
       "ENFP     675\n",
       "ISTP     337\n",
       "ISFP     271\n",
       "ENTJ     231\n",
       "ISTJ     205\n",
       "ENFJ     190\n",
       "ISFJ     166\n",
       "ESTP      89\n",
       "ESFP      48\n",
       "ESFJ      42\n",
       "ESTJ      39\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../data/Kaggle_MBTI.csv')\n",
    "data.type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenenying/miniforge3/envs/chen/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = data.loc[:,['type']]\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "Y = LabelEncoder().fit_transform(label)   # Y before train-test-split\n",
    "label = pd.DataFrame(Y, columns=['LABEL'])\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "labels = to_categorical(label['LABEL'], num_classes=16)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 77994 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "n_most_common_words = 8000\n",
    "max_len = 100   # 保留每筆資料的前500個字\n",
    "tokenizer = Tokenizer(num_words=n_most_common_words, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(data_snow_removeMBTI['posts'].values)\n",
    "sequences = tokenizer.texts_to_sequences(data_snow_removeMBTI['posts'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "X = pad_sequences(sequences, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((6506, 100), (6506, 16), (2169, 100), (2169, 16))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X , labels, test_size=0.25, random_state=42)\n",
    "\n",
    "epochs = 100\n",
    "emb_dim = 128\n",
    "batch_size = 256\n",
    "labels[:2]\n",
    "\n",
    "print((X_train.shape, y_train.shape, X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 100, 128)          128000    \n",
      "                                                                 \n",
      " spatial_dropout1d_2 (Spatia  (None, 100, 128)         0         \n",
      " lDropout1D)                                                     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                1040      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 178,448\n",
      "Trainable params: 178,448\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-23 13:30:26.824512: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - ETA: 0s - loss: 2.7197 - acc: 0.1261 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-23 13:40:16.870941: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 595s 28s/step - loss: 2.7197 - acc: 0.1261 - val_loss: 2.6499 - val_acc: 0.2189\n",
      "Epoch 2/100\n",
      "21/21 [==============================] - 589s 28s/step - loss: 2.4542 - acc: 0.1943 - val_loss: 2.3303 - val_acc: 0.2189\n",
      "Epoch 3/100\n",
      "21/21 [==============================] - 588s 28s/step - loss: 2.3153 - acc: 0.2075 - val_loss: 2.2946 - val_acc: 0.2189\n",
      "Epoch 4/100\n",
      "21/21 [==============================] - 590s 28s/step - loss: 2.2901 - acc: 0.2050 - val_loss: 2.2832 - val_acc: 0.2189\n",
      "Epoch 5/100\n",
      "21/21 [==============================] - 594s 28s/step - loss: 2.2709 - acc: 0.2133 - val_loss: 2.2817 - val_acc: 0.2189\n",
      "Epoch 6/100\n",
      "21/21 [==============================] - 594s 28s/step - loss: 2.2624 - acc: 0.2210 - val_loss: 2.2817 - val_acc: 0.2189\n",
      "Epoch 7/100\n",
      "21/21 [==============================] - 590s 28s/step - loss: 2.2503 - acc: 0.2329 - val_loss: 2.2802 - val_acc: 0.2189\n",
      "Epoch 8/100\n",
      "21/21 [==============================] - 598s 29s/step - loss: 2.2343 - acc: 0.2352 - val_loss: 2.2815 - val_acc: 0.2181\n",
      "Epoch 9/100\n",
      "21/21 [==============================] - 594s 28s/step - loss: 2.2092 - acc: 0.2581 - val_loss: 2.2810 - val_acc: 0.2174\n",
      "Epoch 10/100\n",
      "21/21 [==============================] - 586s 28s/step - loss: 2.1883 - acc: 0.2867 - val_loss: 2.2820 - val_acc: 0.2166\n",
      "Epoch 11/100\n",
      "21/21 [==============================] - 583s 28s/step - loss: 2.1530 - acc: 0.2953 - val_loss: 2.2840 - val_acc: 0.2166\n",
      "Epoch 12/100\n",
      "21/21 [==============================] - 592s 28s/step - loss: 2.1200 - acc: 0.3125 - val_loss: 2.2809 - val_acc: 0.2120\n",
      "Epoch 13/100\n",
      "21/21 [==============================] - 588s 28s/step - loss: 2.0909 - acc: 0.3221 - val_loss: 2.2919 - val_acc: 0.2074\n",
      "Epoch 14/100\n",
      "21/21 [==============================] - 614s 29s/step - loss: 2.0632 - acc: 0.3274 - val_loss: 2.2907 - val_acc: 0.2051\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(1000, emb_dim, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.7))\n",
    "model.add(LSTM(64, dropout=0.7, recurrent_dropout=0.7))\n",
    "model.add(Dense(16, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "print(model.summary())\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',patience=7, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 41s 604ms/step - loss: 2.2766 - acc: 0.2125\n",
      "Test set\n",
      "  Loss: 2.277\n",
      "  Accuracy: 0.213\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(X_test,y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 18:17:17.413036: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 44s 641ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.02163923, 0.07823421, 0.02876924, ..., 0.03419801, 0.02511852,\n",
       "        0.04449378],\n",
       "       [0.02109298, 0.08025363, 0.02836107, ..., 0.03072802, 0.0242075 ,\n",
       "        0.04046316],\n",
       "       [0.02021379, 0.07784317, 0.0282184 , ..., 0.03165273, 0.02338939,\n",
       "        0.0400547 ],\n",
       "       ...,\n",
       "       [0.02045593, 0.08671936, 0.02453822, ..., 0.02723196, 0.0219667 ,\n",
       "        0.03612923],\n",
       "       [0.02202559, 0.08268887, 0.02881961, ..., 0.03254695, 0.02449911,\n",
       "        0.04150951],\n",
       "       [0.02211777, 0.08327577, 0.02706769, ..., 0.02989568, 0.02387635,\n",
       "        0.03850861]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model.predict(X_test)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a537504b26108a9288f9200a20c08832bc40f9b82f2295d4a9841c1a2797efdd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
