{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocess Note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kanko/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/kanko/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/kanko/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/kanko/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data Analysis\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Text Processing \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "\n",
    "# Other\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLTK resource\n",
    "import nltk\n",
    "# nltk.download('punkt')  # for word_tokenize\n",
    "# nltk.download('wordnet')  # for WordNetLemmatizer\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8675, 2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv('../../data/Kaggle_MBTI.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy data for explaining text preprocessing\n",
    "data_copy = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning\n",
    "- 簡單的去除分隔符、超連結、符號、多餘的空格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCleanPost(text):\n",
    "\ttext = re.sub(r'\\|\\|\\|', ' ', text)  # Split by separator\n",
    "\ttext = re.sub(r'http\\S+', ' ', text)  # Replace hyperlink\n",
    "\ttext = re.sub(r\"[A-Za-z]+\\'+\\w+\", ' ', text)  # Handling apostrophe (e.g. you've, there's)\n",
    "\ttext = re.sub('[^0-9a-zA-Z]',' ', text)  # Keep only numbers and alphabets (remove special characters)\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96mBefore cleaning:\n",
      " 'http://www.youtube.com/watch?v=qsXHcwe3krw|||http://41.media.tumblr.com/tumblr_lfouy03PMA1qa1rooo1_500.jpg|||enfp and intj moments  https://www.youtube.com/watch?v=iz7lE1g4XM4  sportscenter not top ten plays  https://www.youtube.com/watch?v=uCdfze1etec  pranks|||What has been the most life-changing experience in your life?|||http://www.youtube.com/watch?v=vXZeYwwRDw8   http://www.youtube.com/watch?v=u8ejam5DP3E  On repeat for most of today.|||May the PerC Experience immerse you.|||The last thing my INFJ friend pos \n",
      "\n",
      "\u001b[94mAfter cleaning:\n",
      "      enfp and intj moments     sportscenter not top ten plays     pranks What has been the most life changing experience in your life         On repeat for most of today  May the PerC Experience immerse you  The last thing my INFJ friend pos\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Example of getCleanPost.\n",
    "input: Top 520 words in data.posts[0]\n",
    "output: getCleanPost(input)\n",
    "'''\n",
    "origi_sentence = data.posts[0][0:520]\n",
    "clean_sentence = getCleanPost(origi_sentence)\n",
    "\n",
    "print('\\033[96mBefore cleaning:\\n',origi_sentence,'\\n')\n",
    "\n",
    "print('\\033[94mAfter cleaning:\\n',clean_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8675/8675 [00:07<00:00, 1126.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>preprocessed</th>\n",
       "      <th>posts_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "      <td>[enfp, intj, moment, sportscent, top, ten, pla...</td>\n",
       "      <td>enfp and intj moments     sportscenter no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "      <td>[find, lack, post, alarm, sex, bore, posit, of...</td>\n",
       "      <td>finding the lack of me in these posts very ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "      <td>[good, one, cours, say, know, bless, cur, abso...</td>\n",
       "      <td>Good one            Of course  to which I say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "      <td>[dear, intp, enjoy, convers, day, esoter, gab,...</td>\n",
       "      <td>Dear INTP    I enjoyed our conversation the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "      <td>[fire, anoth, silli, misconcept, approach, log...</td>\n",
       "      <td>fired    another silly misconception  That ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8670</th>\n",
       "      <td>ISFP</td>\n",
       "      <td>'https://www.youtube.com/watch?v=t8edHB_h908||...</td>\n",
       "      <td>[ixfp, alway, think, cat, fi, dom, reason, esp...</td>\n",
       "      <td>IxFP just because I always think of cats as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8671</th>\n",
       "      <td>ENFP</td>\n",
       "      <td>'So...if this thread already exists someplace ...</td>\n",
       "      <td>[thread, alreadi, exist, someplac, el, heck, d...</td>\n",
       "      <td>So   if this thread already exists someplace ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8672</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'So many questions when i do these things.  I ...</td>\n",
       "      <td>[mani, question, thing, would, take, purpl, pi...</td>\n",
       "      <td>So many questions when i do these things   I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8673</th>\n",
       "      <td>INFP</td>\n",
       "      <td>'I am very conflicted right now when it comes ...</td>\n",
       "      <td>[conflict, right, come, want, child, honest, m...</td>\n",
       "      <td>I am very conflicted right now when it comes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8674</th>\n",
       "      <td>INFP</td>\n",
       "      <td>'It has been too long since I have been on per...</td>\n",
       "      <td>[long, sinc, personalitycaf, although, seem, c...</td>\n",
       "      <td>It has been too long since I have been on per...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8675 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      type                                              posts  \\\n",
       "0     INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...   \n",
       "1     ENTP  'I'm finding the lack of me in these posts ver...   \n",
       "2     INTP  'Good one  _____   https://www.youtube.com/wat...   \n",
       "3     INTJ  'Dear INTP,   I enjoyed our conversation the o...   \n",
       "4     ENTJ  'You're fired.|||That's another silly misconce...   \n",
       "...    ...                                                ...   \n",
       "8670  ISFP  'https://www.youtube.com/watch?v=t8edHB_h908||...   \n",
       "8671  ENFP  'So...if this thread already exists someplace ...   \n",
       "8672  INTP  'So many questions when i do these things.  I ...   \n",
       "8673  INFP  'I am very conflicted right now when it comes ...   \n",
       "8674  INFP  'It has been too long since I have been on per...   \n",
       "\n",
       "                                           preprocessed  \\\n",
       "0     [enfp, intj, moment, sportscent, top, ten, pla...   \n",
       "1     [find, lack, post, alarm, sex, bore, posit, of...   \n",
       "2     [good, one, cours, say, know, bless, cur, abso...   \n",
       "3     [dear, intp, enjoy, convers, day, esoter, gab,...   \n",
       "4     [fire, anoth, silli, misconcept, approach, log...   \n",
       "...                                                 ...   \n",
       "8670  [ixfp, alway, think, cat, fi, dom, reason, esp...   \n",
       "8671  [thread, alreadi, exist, someplac, el, heck, d...   \n",
       "8672  [mani, question, thing, would, take, purpl, pi...   \n",
       "8673  [conflict, right, come, want, child, honest, m...   \n",
       "8674  [long, sinc, personalitycaf, although, seem, c...   \n",
       "\n",
       "                                            posts_clean  \n",
       "0          enfp and intj moments     sportscenter no...  \n",
       "1        finding the lack of me in these posts very ...  \n",
       "2      Good one            Of course  to which I say...  \n",
       "3      Dear INTP    I enjoyed our conversation the o...  \n",
       "4        fired    another silly misconception  That ...  \n",
       "...                                                 ...  \n",
       "8670     IxFP just because I always think of cats as...  \n",
       "8671   So   if this thread already exists someplace ...  \n",
       "8672   So many questions when i do these things   I ...  \n",
       "8673   I am very conflicted right now when it comes ...  \n",
       "8674   It has been too long since I have been on per...  \n",
       "\n",
       "[8675 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply getCleanPost to all training data\n",
    "tqdm.pandas()  # Progress bar\n",
    "data_copy['posts_clean'] = data_copy['posts'].progress_apply(getCleanPost)\n",
    "data_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization & Remove stop words\n",
    "- 轉小寫\n",
    "- 切詞\n",
    "- 移除 Stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### getCleanToken()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words\n",
      " ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Stop word list\n",
    "stop_words = stopwords.words('english')\n",
    "print('Stop words\\n',stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCleanToken(text):\n",
    "\t# getCleanPost\n",
    "\ttext = re.sub(r'\\|\\|\\|', r' ', text)\n",
    "\ttext = re.sub(r'http\\S+', r'', text)\n",
    "\ttext = re.sub('[^0-9a-zA-Z]',' ', text)\n",
    "\ttext = re.sub(' +', ' ', text)\n",
    "\t# Add \"Tokenization\" and remove stopword\n",
    "\ttext = text.lower()\n",
    "\ttokens = word_tokenize(text)\n",
    "\tfiltered_tokens = [w for w in tokens if not w in stop_words]\n",
    "\treturn filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 829 tokens\n",
      "\n",
      "After removing stop words: 491 tokens\n",
      "\n",
      "Removed words: ['just', 'do', 'of', 'and', 'so', 'he', 'them', 'while', 'when', 'as', 'in', 'not', 'at', 'with', 'each', 'it', 'some', 'other', 'on', 'only', 'yours', 'a', 'how', 'have', 'will', 'your', 'few', 'or', 'about', 'is', 'which', 'me', 'o', 'any', 'why', 'once', 'here', 'up', 'because', 'this', 'they', 'did', 'him', 'am', 'all', 'has', 'to', 'who', 'very', 'more', 'what', 'are', 'were', 'an', 'you', 'too', 'his', 'no', 'be', 'then', 'we', 'out', 'where', 'can', 'that', 'if', 'their', 'from', 'for', 'my', 'her', 'now', 'the', 'those']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Example of the added part in getCleanToken.\n",
    "Referred to the paragraph # Add \"Tokenization\" and remove stopword\n",
    "input: getCleanPost(user #1228)\n",
    "output: getCleanToken(user #1228)\n",
    "'''\n",
    "clean_post = getCleanPost(data_copy.posts[1228])\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(clean_post)\n",
    "print(f'Original: {len(tokens)} tokens\\n')\n",
    "\n",
    "# Stop words\n",
    "filtered_tokens = [w for w in tokens if not w in stop_words]\n",
    "print(f'After removing stop words: {len(filtered_tokens)} tokens\\n')\n",
    "\n",
    "# Check removed words\n",
    "print(f'Removed words: {list(set(tokens).difference(set(filtered_tokens)))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8675/8675 [00:55<00:00, 157.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# Apply getCleanToken to all training data\n",
    "tqdm.pandas()  # Progress bar\n",
    "data_copy['tokens_clean'] = data_copy['posts'].progress_apply(getCleanToken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>preprocessed</th>\n",
       "      <th>posts_clean</th>\n",
       "      <th>tokens_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "      <td>[enfp, intj, moment, sportscent, top, ten, pla...</td>\n",
       "      <td>enfp and intj moments     sportscenter no...</td>\n",
       "      <td>[enfp, intj, moments, sportscenter, top, ten, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "      <td>[find, lack, post, alarm, sex, bore, posit, of...</td>\n",
       "      <td>finding the lack of me in these posts very ...</td>\n",
       "      <td>[finding, lack, posts, alarming, sex, boring, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "      <td>[good, one, cours, say, know, bless, cur, abso...</td>\n",
       "      <td>Good one            Of course  to which I say...</td>\n",
       "      <td>[good, one, course, say, know, blessing, curse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "      <td>[dear, intp, enjoy, convers, day, esoter, gab,...</td>\n",
       "      <td>Dear INTP    I enjoyed our conversation the o...</td>\n",
       "      <td>[dear, intp, enjoyed, conversation, day, esote...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "      <td>[fire, anoth, silli, misconcept, approach, log...</td>\n",
       "      <td>fired    another silly misconception  That ...</td>\n",
       "      <td>[fired, another, silly, misconception, approac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8670</th>\n",
       "      <td>ISFP</td>\n",
       "      <td>'https://www.youtube.com/watch?v=t8edHB_h908||...</td>\n",
       "      <td>[ixfp, alway, think, cat, fi, dom, reason, esp...</td>\n",
       "      <td>IxFP just because I always think of cats as...</td>\n",
       "      <td>[ixfp, always, think, cats, fi, doms, reason, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8671</th>\n",
       "      <td>ENFP</td>\n",
       "      <td>'So...if this thread already exists someplace ...</td>\n",
       "      <td>[thread, alreadi, exist, someplac, el, heck, d...</td>\n",
       "      <td>So   if this thread already exists someplace ...</td>\n",
       "      <td>[thread, already, exists, someplace, else, hec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8672</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'So many questions when i do these things.  I ...</td>\n",
       "      <td>[mani, question, thing, would, take, purpl, pi...</td>\n",
       "      <td>So many questions when i do these things   I ...</td>\n",
       "      <td>[many, questions, things, would, take, purple,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8673</th>\n",
       "      <td>INFP</td>\n",
       "      <td>'I am very conflicted right now when it comes ...</td>\n",
       "      <td>[conflict, right, come, want, child, honest, m...</td>\n",
       "      <td>I am very conflicted right now when it comes ...</td>\n",
       "      <td>[conflicted, right, comes, wanting, children, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8674</th>\n",
       "      <td>INFP</td>\n",
       "      <td>'It has been too long since I have been on per...</td>\n",
       "      <td>[long, sinc, personalitycaf, although, seem, c...</td>\n",
       "      <td>It has been too long since I have been on per...</td>\n",
       "      <td>[long, since, personalitycafe, although, seem,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8675 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      type                                              posts  \\\n",
       "0     INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...   \n",
       "1     ENTP  'I'm finding the lack of me in these posts ver...   \n",
       "2     INTP  'Good one  _____   https://www.youtube.com/wat...   \n",
       "3     INTJ  'Dear INTP,   I enjoyed our conversation the o...   \n",
       "4     ENTJ  'You're fired.|||That's another silly misconce...   \n",
       "...    ...                                                ...   \n",
       "8670  ISFP  'https://www.youtube.com/watch?v=t8edHB_h908||...   \n",
       "8671  ENFP  'So...if this thread already exists someplace ...   \n",
       "8672  INTP  'So many questions when i do these things.  I ...   \n",
       "8673  INFP  'I am very conflicted right now when it comes ...   \n",
       "8674  INFP  'It has been too long since I have been on per...   \n",
       "\n",
       "                                           preprocessed  \\\n",
       "0     [enfp, intj, moment, sportscent, top, ten, pla...   \n",
       "1     [find, lack, post, alarm, sex, bore, posit, of...   \n",
       "2     [good, one, cours, say, know, bless, cur, abso...   \n",
       "3     [dear, intp, enjoy, convers, day, esoter, gab,...   \n",
       "4     [fire, anoth, silli, misconcept, approach, log...   \n",
       "...                                                 ...   \n",
       "8670  [ixfp, alway, think, cat, fi, dom, reason, esp...   \n",
       "8671  [thread, alreadi, exist, someplac, el, heck, d...   \n",
       "8672  [mani, question, thing, would, take, purpl, pi...   \n",
       "8673  [conflict, right, come, want, child, honest, m...   \n",
       "8674  [long, sinc, personalitycaf, although, seem, c...   \n",
       "\n",
       "                                            posts_clean  \\\n",
       "0          enfp and intj moments     sportscenter no...   \n",
       "1        finding the lack of me in these posts very ...   \n",
       "2      Good one            Of course  to which I say...   \n",
       "3      Dear INTP    I enjoyed our conversation the o...   \n",
       "4        fired    another silly misconception  That ...   \n",
       "...                                                 ...   \n",
       "8670     IxFP just because I always think of cats as...   \n",
       "8671   So   if this thread already exists someplace ...   \n",
       "8672   So many questions when i do these things   I ...   \n",
       "8673   I am very conflicted right now when it comes ...   \n",
       "8674   It has been too long since I have been on per...   \n",
       "\n",
       "                                           tokens_clean  \n",
       "0     [enfp, intj, moments, sportscenter, top, ten, ...  \n",
       "1     [finding, lack, posts, alarming, sex, boring, ...  \n",
       "2     [good, one, course, say, know, blessing, curse...  \n",
       "3     [dear, intp, enjoyed, conversation, day, esote...  \n",
       "4     [fired, another, silly, misconception, approac...  \n",
       "...                                                 ...  \n",
       "8670  [ixfp, always, think, cats, fi, doms, reason, ...  \n",
       "8671  [thread, already, exists, someplace, else, hec...  \n",
       "8672  [many, questions, things, would, take, purple,...  \n",
       "8673  [conflicted, right, comes, wanting, children, ...  \n",
       "8674  [long, since, personalitycafe, although, seem,...  \n",
       "\n",
       "[8675 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>preprocessed</th>\n",
       "      <th>posts_clean</th>\n",
       "      <th>tokens_clean</th>\n",
       "      <th>Words count after getCleanPost</th>\n",
       "      <th>Words count after getCleanToken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "      <td>[enfp, intj, moment, sportscent, top, ten, pla...</td>\n",
       "      <td>enfp and intj moments     sportscenter no...</td>\n",
       "      <td>[enfp, intj, moments, sportscenter, top, ten, ...</td>\n",
       "      <td>570</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "      <td>[find, lack, post, alarm, sex, bore, posit, of...</td>\n",
       "      <td>finding the lack of me in these posts very ...</td>\n",
       "      <td>[finding, lack, posts, alarming, sex, boring, ...</td>\n",
       "      <td>1166</td>\n",
       "      <td>572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "      <td>[good, one, cours, say, know, bless, cur, abso...</td>\n",
       "      <td>Good one            Of course  to which I say...</td>\n",
       "      <td>[good, one, course, say, know, blessing, curse...</td>\n",
       "      <td>841</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "      <td>[dear, intp, enjoy, convers, day, esoter, gab,...</td>\n",
       "      <td>Dear INTP    I enjoyed our conversation the o...</td>\n",
       "      <td>[dear, intp, enjoyed, conversation, day, esote...</td>\n",
       "      <td>1068</td>\n",
       "      <td>542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "      <td>[fire, anoth, silli, misconcept, approach, log...</td>\n",
       "      <td>fired    another silly misconception  That ...</td>\n",
       "      <td>[fired, another, silly, misconception, approac...</td>\n",
       "      <td>987</td>\n",
       "      <td>484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8670</th>\n",
       "      <td>ISFP</td>\n",
       "      <td>'https://www.youtube.com/watch?v=t8edHB_h908||...</td>\n",
       "      <td>[ixfp, alway, think, cat, fi, dom, reason, esp...</td>\n",
       "      <td>IxFP just because I always think of cats as...</td>\n",
       "      <td>[ixfp, always, think, cats, fi, doms, reason, ...</td>\n",
       "      <td>798</td>\n",
       "      <td>406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8671</th>\n",
       "      <td>ENFP</td>\n",
       "      <td>'So...if this thread already exists someplace ...</td>\n",
       "      <td>[thread, alreadi, exist, someplac, el, heck, d...</td>\n",
       "      <td>So   if this thread already exists someplace ...</td>\n",
       "      <td>[thread, already, exists, someplace, else, hec...</td>\n",
       "      <td>1329</td>\n",
       "      <td>630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8672</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'So many questions when i do these things.  I ...</td>\n",
       "      <td>[mani, question, thing, would, take, purpl, pi...</td>\n",
       "      <td>So many questions when i do these things   I ...</td>\n",
       "      <td>[many, questions, things, would, take, purple,...</td>\n",
       "      <td>978</td>\n",
       "      <td>503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8673</th>\n",
       "      <td>INFP</td>\n",
       "      <td>'I am very conflicted right now when it comes ...</td>\n",
       "      <td>[conflict, right, come, want, child, honest, m...</td>\n",
       "      <td>I am very conflicted right now when it comes ...</td>\n",
       "      <td>[conflicted, right, comes, wanting, children, ...</td>\n",
       "      <td>1684</td>\n",
       "      <td>778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8674</th>\n",
       "      <td>INFP</td>\n",
       "      <td>'It has been too long since I have been on per...</td>\n",
       "      <td>[long, sinc, personalitycaf, although, seem, c...</td>\n",
       "      <td>It has been too long since I have been on per...</td>\n",
       "      <td>[long, since, personalitycafe, although, seem,...</td>\n",
       "      <td>1350</td>\n",
       "      <td>617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8675 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      type                                              posts  \\\n",
       "0     INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...   \n",
       "1     ENTP  'I'm finding the lack of me in these posts ver...   \n",
       "2     INTP  'Good one  _____   https://www.youtube.com/wat...   \n",
       "3     INTJ  'Dear INTP,   I enjoyed our conversation the o...   \n",
       "4     ENTJ  'You're fired.|||That's another silly misconce...   \n",
       "...    ...                                                ...   \n",
       "8670  ISFP  'https://www.youtube.com/watch?v=t8edHB_h908||...   \n",
       "8671  ENFP  'So...if this thread already exists someplace ...   \n",
       "8672  INTP  'So many questions when i do these things.  I ...   \n",
       "8673  INFP  'I am very conflicted right now when it comes ...   \n",
       "8674  INFP  'It has been too long since I have been on per...   \n",
       "\n",
       "                                           preprocessed  \\\n",
       "0     [enfp, intj, moment, sportscent, top, ten, pla...   \n",
       "1     [find, lack, post, alarm, sex, bore, posit, of...   \n",
       "2     [good, one, cours, say, know, bless, cur, abso...   \n",
       "3     [dear, intp, enjoy, convers, day, esoter, gab,...   \n",
       "4     [fire, anoth, silli, misconcept, approach, log...   \n",
       "...                                                 ...   \n",
       "8670  [ixfp, alway, think, cat, fi, dom, reason, esp...   \n",
       "8671  [thread, alreadi, exist, someplac, el, heck, d...   \n",
       "8672  [mani, question, thing, would, take, purpl, pi...   \n",
       "8673  [conflict, right, come, want, child, honest, m...   \n",
       "8674  [long, sinc, personalitycaf, although, seem, c...   \n",
       "\n",
       "                                            posts_clean  \\\n",
       "0          enfp and intj moments     sportscenter no...   \n",
       "1        finding the lack of me in these posts very ...   \n",
       "2      Good one            Of course  to which I say...   \n",
       "3      Dear INTP    I enjoyed our conversation the o...   \n",
       "4        fired    another silly misconception  That ...   \n",
       "...                                                 ...   \n",
       "8670     IxFP just because I always think of cats as...   \n",
       "8671   So   if this thread already exists someplace ...   \n",
       "8672   So many questions when i do these things   I ...   \n",
       "8673   I am very conflicted right now when it comes ...   \n",
       "8674   It has been too long since I have been on per...   \n",
       "\n",
       "                                           tokens_clean  \\\n",
       "0     [enfp, intj, moments, sportscenter, top, ten, ...   \n",
       "1     [finding, lack, posts, alarming, sex, boring, ...   \n",
       "2     [good, one, course, say, know, blessing, curse...   \n",
       "3     [dear, intp, enjoyed, conversation, day, esote...   \n",
       "4     [fired, another, silly, misconception, approac...   \n",
       "...                                                 ...   \n",
       "8670  [ixfp, always, think, cats, fi, doms, reason, ...   \n",
       "8671  [thread, already, exists, someplace, else, hec...   \n",
       "8672  [many, questions, things, would, take, purple,...   \n",
       "8673  [conflicted, right, comes, wanting, children, ...   \n",
       "8674  [long, since, personalitycafe, although, seem,...   \n",
       "\n",
       "      Words count after getCleanPost  Words count after getCleanToken  \n",
       "0                                570                              318  \n",
       "1                               1166                              572  \n",
       "2                                841                              445  \n",
       "3                               1068                              542  \n",
       "4                                987                              484  \n",
       "...                              ...                              ...  \n",
       "8670                             798                              406  \n",
       "8671                            1329                              630  \n",
       "8672                             978                              503  \n",
       "8673                            1684                              778  \n",
       "8674                            1350                              617  \n",
       "\n",
       "[8675 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Statistics\n",
    "data_copy['Words count after getCleanPost'] = data_copy['posts_clean'].apply(lambda n: len(n.split()))\n",
    "data_copy['Words count after getCleanToken'] = data_copy['tokens_clean'].str.len()\n",
    "data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words count after getCleanPost</th>\n",
       "      <th>Words count after getCleanToken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8675.000000</td>\n",
       "      <td>8675.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1231.201960</td>\n",
       "      <td>614.928646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>306.412055</td>\n",
       "      <td>142.514306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1055.500000</td>\n",
       "      <td>536.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1283.000000</td>\n",
       "      <td>640.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1456.000000</td>\n",
       "      <td>719.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1880.000000</td>\n",
       "      <td>927.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Words count after getCleanPost  Words count after getCleanToken\n",
       "count                     8675.000000                      8675.000000\n",
       "mean                      1231.201960                       614.928646\n",
       "std                        306.412055                       142.514306\n",
       "min                          4.000000                         3.000000\n",
       "25%                       1055.500000                       536.000000\n",
       "50%                       1283.000000                       640.000000\n",
       "75%                       1456.000000                       719.000000\n",
       "max                       1880.000000                       927.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_copy.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3: Stemming and Lemmatization\n",
    "- 比較 PorterStemmer 與 SnowballStemmer 的結果\n",
    "- 用 WordNetLemmatizer 進行 Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocessor(text:str, stemmer: str='Snowball', remove_mbti: bool=False) -> list:\n",
    "\t'''\n",
    "\tInput: str\n",
    "\tOutput: list\n",
    "\t\tPreprocessed tokens\n",
    "\tstemmer: str\n",
    "\t\tCan be 'Snowball' or 'Porter'. Default is Snowball.\n",
    "\tremove_mbti: bool\n",
    "\t\tRemove MBTI keywords like INTJ, ENFP, etc. Default is False.(Keep MBTI keywords.)\n",
    "\t'''\n",
    "\t# Cleaning\n",
    "\ttext = re.sub(r'\\|\\|\\|', ' ', text)  # Split by separator\n",
    "\ttext = re.sub(r'http\\S+', ' ', text)  # Replace hyperlink\n",
    "\ttext = re.sub(r\"[A-Za-z]+\\'+\\w+\", ' ', text)  # Handling apostrophe (e.g. you've, there's)\n",
    "\ttext = re.sub('[^0-9a-zA-Z]',' ', text)  # Keep only numbers and alphabets (remove special characters)\n",
    "\ttext = text.lower()\n",
    "\tif remove_mbti == True:\n",
    "\t\ttext = re.sub('intj|intp|entj|entp|infp|enfj|enfp|istj|isfj|estj|esfj|istp|isfp|estp|esfp|infj', '', text)\n",
    "  \t# Tokenization\n",
    "\ttokens = word_tokenize(text)\n",
    "\tfiltered_tokens = [w for w in tokens if not w in stopwords.words('english')]  # Remove stopwords\n",
    "\t# Stemming\n",
    "\tstemmer_ = SnowballStemmer(\"english\")\n",
    "\tif stemmer == 'Porter|porter':\n",
    "\t\tstemmer_ = PorterStemmer()\n",
    "\tif stemmer not in ['Snowball', 'snowball', 'Porter', 'porter']:\n",
    "\t\traise ValueError(\"Please check passed argument: stemmer must be 'Snowball' or 'Porter'\")\n",
    "\tstemmed = [stemmer_.stem(t) for t in filtered_tokens]\n",
    "\t# Lemmatizing\n",
    "\tlemma = WordNetLemmatizer()\n",
    "\tlemmatized = [lemma.lemmatize(t) for t in stemmed]\n",
    "\treturn lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Example of the added part in Preprocessor.\n",
    "Referred to the paragraph: # Add \"Stemming\" and \"Lemmatization\"\n",
    "input: getCleanToken(user #1228)\n",
    "output: Preprocessor(user #1228)\n",
    "'''\n",
    "clean_token = getCleanToken(data_copy.posts[1228])\n",
    "# Initiate\n",
    "stemmer_ps = PorterStemmer()\n",
    "stemmer_ss = SnowballStemmer(\"english\") \n",
    "lemma = WordNetLemmatizer()\n",
    "# Stemming\n",
    "stemmed_ps = [stemmer_ps.stem(t) for t in clean_token]\n",
    "stemmed_ss = [stemmer_ss.stem(t) for t in clean_token]\n",
    "# Lemmatizing\n",
    "lemmatized_ps = [lemma.lemmatize(t) for t in stemmed_ps]\n",
    "lemmatized_ss = [lemma.lemmatize(t) for t in stemmed_ss]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare different Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original(clean_token)</th>\n",
       "      <th>PorterStemmer</th>\n",
       "      <th>SnowballStemmer</th>\n",
       "      <th>Lemma with PorterStemmer</th>\n",
       "      <th>Lemma with SnowballStemmer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mandarin</td>\n",
       "      <td>mandarin</td>\n",
       "      <td>mandarin</td>\n",
       "      <td>mandarin</td>\n",
       "      <td>mandarin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>speakers</td>\n",
       "      <td>speaker</td>\n",
       "      <td>speaker</td>\n",
       "      <td>speaker</td>\n",
       "      <td>speaker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>receive</td>\n",
       "      <td>receiv</td>\n",
       "      <td>receiv</td>\n",
       "      <td>receiv</td>\n",
       "      <td>receiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>education</td>\n",
       "      <td>educ</td>\n",
       "      <td>educ</td>\n",
       "      <td>educ</td>\n",
       "      <td>educ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>canada</td>\n",
       "      <td>canada</td>\n",
       "      <td>canada</td>\n",
       "      <td>canada</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>since</td>\n",
       "      <td>sinc</td>\n",
       "      <td>sinc</td>\n",
       "      <td>sinc</td>\n",
       "      <td>sinc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>thanks</td>\n",
       "      <td>thank</td>\n",
       "      <td>thank</td>\n",
       "      <td>thank</td>\n",
       "      <td>thank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bellisaurius</td>\n",
       "      <td>bellisauriu</td>\n",
       "      <td>bellisaurius</td>\n",
       "      <td>bellisauriu</td>\n",
       "      <td>bellisaurius</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>appreciate</td>\n",
       "      <td>appreci</td>\n",
       "      <td>appreci</td>\n",
       "      <td>appreci</td>\n",
       "      <td>appreci</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Original(clean_token) PorterStemmer SnowballStemmer  \\\n",
       "0              mandarin      mandarin        mandarin   \n",
       "1              speakers       speaker         speaker   \n",
       "2               receive        receiv          receiv   \n",
       "3             education          educ            educ   \n",
       "4                canada        canada          canada   \n",
       "5                 since          sinc            sinc   \n",
       "6                    13            13              13   \n",
       "7                thanks         thank           thank   \n",
       "8          bellisaurius   bellisauriu    bellisaurius   \n",
       "9            appreciate       appreci         appreci   \n",
       "\n",
       "  Lemma with PorterStemmer Lemma with SnowballStemmer  \n",
       "0                 mandarin                   mandarin  \n",
       "1                  speaker                    speaker  \n",
       "2                   receiv                     receiv  \n",
       "3                     educ                       educ  \n",
       "4                   canada                     canada  \n",
       "5                     sinc                       sinc  \n",
       "6                       13                         13  \n",
       "7                    thank                      thank  \n",
       "8              bellisauriu               bellisaurius  \n",
       "9                  appreci                    appreci  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare different 'Stemmer' and 'Lemmatizer'\n",
    "df_stle = pd.DataFrame(\n",
    "          list(zip(clean_token, stemmed_ps, stemmed_ss, lemmatized_ps, lemmatized_ss)),\n",
    "          columns =['Original(clean_token)', 'PorterStemmer', 'SnowballStemmer', 'Lemma with PorterStemmer', 'Lemma with SnowballStemmer']) \n",
    "df_stle.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PorterStemmer and SnowballStemmer has  15 / 444  different tokens in user #1228's posts.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original(clean_token)</th>\n",
       "      <th>PorterStemmer</th>\n",
       "      <th>SnowballStemmer</th>\n",
       "      <th>Lemma with PorterStemmer</th>\n",
       "      <th>Lemma with SnowballStemmer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bellisaurius</td>\n",
       "      <td>bellisauriu</td>\n",
       "      <td>bellisaurius</td>\n",
       "      <td>bellisauriu</td>\n",
       "      <td>bellisaurius</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>kindly</td>\n",
       "      <td>kindli</td>\n",
       "      <td>kind</td>\n",
       "      <td>kindli</td>\n",
       "      <td>kind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>yes</td>\n",
       "      <td>ye</td>\n",
       "      <td>yes</td>\n",
       "      <td>ye</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>yes</td>\n",
       "      <td>ye</td>\n",
       "      <td>yes</td>\n",
       "      <td>ye</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>yes</td>\n",
       "      <td>ye</td>\n",
       "      <td>yes</td>\n",
       "      <td>ye</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>saurus</td>\n",
       "      <td>sauru</td>\n",
       "      <td>saurus</td>\n",
       "      <td>sauru</td>\n",
       "      <td>saurus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>dos</td>\n",
       "      <td>do</td>\n",
       "      <td>dos</td>\n",
       "      <td>do</td>\n",
       "      <td>do</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>pros</td>\n",
       "      <td>pro</td>\n",
       "      <td>pros</td>\n",
       "      <td>pro</td>\n",
       "      <td>pro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>exactly</td>\n",
       "      <td>exactli</td>\n",
       "      <td>exact</td>\n",
       "      <td>exactli</td>\n",
       "      <td>exact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>dos</td>\n",
       "      <td>do</td>\n",
       "      <td>dos</td>\n",
       "      <td>do</td>\n",
       "      <td>do</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>dos</td>\n",
       "      <td>do</td>\n",
       "      <td>dos</td>\n",
       "      <td>do</td>\n",
       "      <td>do</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>dos</td>\n",
       "      <td>do</td>\n",
       "      <td>dos</td>\n",
       "      <td>do</td>\n",
       "      <td>do</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>dos</td>\n",
       "      <td>do</td>\n",
       "      <td>dos</td>\n",
       "      <td>do</td>\n",
       "      <td>do</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>dos</td>\n",
       "      <td>do</td>\n",
       "      <td>dos</td>\n",
       "      <td>do</td>\n",
       "      <td>do</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>communication</td>\n",
       "      <td>commun</td>\n",
       "      <td>communic</td>\n",
       "      <td>commun</td>\n",
       "      <td>communic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Original(clean_token) PorterStemmer SnowballStemmer  \\\n",
       "8            bellisaurius   bellisauriu    bellisaurius   \n",
       "10                 kindly        kindli            kind   \n",
       "41                    yes            ye             yes   \n",
       "46                    yes            ye             yes   \n",
       "157                   yes            ye             yes   \n",
       "161                saurus         sauru          saurus   \n",
       "291                   dos            do             dos   \n",
       "304                  pros           pro            pros   \n",
       "318               exactly       exactli           exact   \n",
       "382                   dos            do             dos   \n",
       "387                   dos            do             dos   \n",
       "399                   dos            do             dos   \n",
       "409                   dos            do             dos   \n",
       "414                   dos            do             dos   \n",
       "422         communication        commun        communic   \n",
       "\n",
       "    Lemma with PorterStemmer Lemma with SnowballStemmer  \n",
       "8                bellisauriu               bellisaurius  \n",
       "10                    kindli                       kind  \n",
       "41                        ye                        yes  \n",
       "46                        ye                        yes  \n",
       "157                       ye                        yes  \n",
       "161                    sauru                     saurus  \n",
       "291                       do                         do  \n",
       "304                      pro                        pro  \n",
       "318                  exactli                      exact  \n",
       "382                       do                         do  \n",
       "387                       do                         do  \n",
       "399                       do                         do  \n",
       "409                       do                         do  \n",
       "414                       do                         do  \n",
       "422                   commun                   communic  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "diff_result = df_stle.query('PorterStemmer != SnowballStemmer')\n",
    "print(f'The PorterStemmer and SnowballStemmer has\\\n",
    "  {diff_result.shape[0]} / {df_stle.shape[0]}\\\n",
    "  different tokens in user #1228\\'s posts.')\n",
    "diff_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4: 各步驟的比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>preprocessed</th>\n",
       "      <th>posts_clean</th>\n",
       "      <th>tokens_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "      <td>[enfp, intj, moment, sportscent, top, ten, pla...</td>\n",
       "      <td>enfp and intj moments     sportscenter no...</td>\n",
       "      <td>[enfp, intj, moments, sportscenter, top, ten, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "      <td>[find, lack, post, alarm, sex, bore, posit, of...</td>\n",
       "      <td>finding the lack of me in these posts very ...</td>\n",
       "      <td>[finding, lack, posts, alarming, sex, boring, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "      <td>[good, one, cours, say, know, bless, cur, abso...</td>\n",
       "      <td>Good one            Of course  to which I say...</td>\n",
       "      <td>[good, one, course, say, know, blessing, curse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "      <td>[dear, intp, enjoy, convers, day, esoter, gab,...</td>\n",
       "      <td>Dear INTP    I enjoyed our conversation the o...</td>\n",
       "      <td>[dear, intp, enjoyed, conversation, day, esote...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "      <td>[fire, anoth, silli, misconcept, approach, log...</td>\n",
       "      <td>fired    another silly misconception  That ...</td>\n",
       "      <td>[fired, another, silly, misconception, approac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8670</th>\n",
       "      <td>ISFP</td>\n",
       "      <td>'https://www.youtube.com/watch?v=t8edHB_h908||...</td>\n",
       "      <td>[ixfp, alway, think, cat, fi, dom, reason, esp...</td>\n",
       "      <td>IxFP just because I always think of cats as...</td>\n",
       "      <td>[ixfp, always, think, cats, fi, doms, reason, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8671</th>\n",
       "      <td>ENFP</td>\n",
       "      <td>'So...if this thread already exists someplace ...</td>\n",
       "      <td>[thread, alreadi, exist, someplac, el, heck, d...</td>\n",
       "      <td>So   if this thread already exists someplace ...</td>\n",
       "      <td>[thread, already, exists, someplace, else, hec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8672</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'So many questions when i do these things.  I ...</td>\n",
       "      <td>[mani, question, thing, would, take, purpl, pi...</td>\n",
       "      <td>So many questions when i do these things   I ...</td>\n",
       "      <td>[many, questions, things, would, take, purple,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8673</th>\n",
       "      <td>INFP</td>\n",
       "      <td>'I am very conflicted right now when it comes ...</td>\n",
       "      <td>[conflict, right, come, want, child, honest, m...</td>\n",
       "      <td>I am very conflicted right now when it comes ...</td>\n",
       "      <td>[conflicted, right, comes, wanting, children, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8674</th>\n",
       "      <td>INFP</td>\n",
       "      <td>'It has been too long since I have been on per...</td>\n",
       "      <td>[long, sinc, personalitycaf, although, seem, c...</td>\n",
       "      <td>It has been too long since I have been on per...</td>\n",
       "      <td>[long, since, personalitycafe, although, seem,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8675 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      type                                              posts  \\\n",
       "0     INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...   \n",
       "1     ENTP  'I'm finding the lack of me in these posts ver...   \n",
       "2     INTP  'Good one  _____   https://www.youtube.com/wat...   \n",
       "3     INTJ  'Dear INTP,   I enjoyed our conversation the o...   \n",
       "4     ENTJ  'You're fired.|||That's another silly misconce...   \n",
       "...    ...                                                ...   \n",
       "8670  ISFP  'https://www.youtube.com/watch?v=t8edHB_h908||...   \n",
       "8671  ENFP  'So...if this thread already exists someplace ...   \n",
       "8672  INTP  'So many questions when i do these things.  I ...   \n",
       "8673  INFP  'I am very conflicted right now when it comes ...   \n",
       "8674  INFP  'It has been too long since I have been on per...   \n",
       "\n",
       "                                           preprocessed  \\\n",
       "0     [enfp, intj, moment, sportscent, top, ten, pla...   \n",
       "1     [find, lack, post, alarm, sex, bore, posit, of...   \n",
       "2     [good, one, cours, say, know, bless, cur, abso...   \n",
       "3     [dear, intp, enjoy, convers, day, esoter, gab,...   \n",
       "4     [fire, anoth, silli, misconcept, approach, log...   \n",
       "...                                                 ...   \n",
       "8670  [ixfp, alway, think, cat, fi, dom, reason, esp...   \n",
       "8671  [thread, alreadi, exist, someplac, el, heck, d...   \n",
       "8672  [mani, question, thing, would, take, purpl, pi...   \n",
       "8673  [conflict, right, come, want, child, honest, m...   \n",
       "8674  [long, sinc, personalitycaf, although, seem, c...   \n",
       "\n",
       "                                            posts_clean  \\\n",
       "0          enfp and intj moments     sportscenter no...   \n",
       "1        finding the lack of me in these posts very ...   \n",
       "2      Good one            Of course  to which I say...   \n",
       "3      Dear INTP    I enjoyed our conversation the o...   \n",
       "4        fired    another silly misconception  That ...   \n",
       "...                                                 ...   \n",
       "8670     IxFP just because I always think of cats as...   \n",
       "8671   So   if this thread already exists someplace ...   \n",
       "8672   So many questions when i do these things   I ...   \n",
       "8673   I am very conflicted right now when it comes ...   \n",
       "8674   It has been too long since I have been on per...   \n",
       "\n",
       "                                           tokens_clean  \n",
       "0     [enfp, intj, moments, sportscenter, top, ten, ...  \n",
       "1     [finding, lack, posts, alarming, sex, boring, ...  \n",
       "2     [good, one, course, say, know, blessing, curse...  \n",
       "3     [dear, intp, enjoyed, conversation, day, esote...  \n",
       "4     [fired, another, silly, misconception, approac...  \n",
       "...                                                 ...  \n",
       "8670  [ixfp, always, think, cats, fi, doms, reason, ...  \n",
       "8671  [thread, already, exists, someplace, else, hec...  \n",
       "8672  [many, questions, things, would, take, purple,...  \n",
       "8673  [conflicted, right, comes, wanting, children, ...  \n",
       "8674  [long, since, personalitycafe, although, seem,...  \n",
       "\n",
       "[8675 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_copy.drop(data_copy.columns[[5,6]],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input (800 words):\n",
      "'http://www.youtube.com/watch?v=qsXHcwe3krw|||http://41.media.tumblr.com/tumblr_lfouy03PMA1qa1rooo1_500.jpg|||enfp and intj moments  https://www.youtube.com/watch?v=iz7lE1g4XM4  sportscenter not top ten plays  https://www.youtube.com/watch?v=uCdfze1etec  pranks|||What has been the most life-changing experience in your life?|||http://www.youtube.com/watch?v=vXZeYwwRDw8   http://www.youtube.com/watch?v=u8ejam5DP3E  On repeat for most of today.|||May the PerC Experience immerse you.|||The last thing my INFJ friend posted on his facebook before committing suicide the next day. Rest in peace~   http://vimeo.com/22842206|||Hello ENFJ7. Sorry to hear of your distress. It's only natural for a relationship to not be perfection all the time in every moment of existence. Try to figure the hard times ...\n"
     ]
    }
   ],
   "source": [
    "print(f'Input (800 words):\\n{data_copy.posts[0][:800]}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "['enfp', 'intj', 'moment', 'sportscent', 'top', 'ten', 'play', 'prank', 'life', 'chang', 'experi', 'life', 'repeat', 'today', 'may', 'perc', 'experi', 'immers', 'last', 'thing', 'infj', 'friend', 'post', 'facebook', 'commit', 'suicid', 'next', 'day', 'rest', 'peac', 'hello', 'enfj7', 'sorri', 'hear', 'distress', 'natur', 'relationship', 'perfect', 'time', 'everi', 'moment', 'exist', 'tri', 'figur', 'hard', 'time', 'time', 'growth', '84389', '84390', 'welcom', 'stuff', 'game', 'set', 'match', 'prozac', 'wellbrutin', 'least', 'thirti', 'minut', 'move', 'leg', 'mean', 'move', 'sit', 'desk', 'chair', 'weed', 'moder', 'mayb', 'tri', 'edibl', 'healthier', 'altern', 'basic', 'come', 'three', 'item', 'determin', 'type', 'whichev', 'type', 'want', 'would', 'like', 'use', 'given', 'type', 'cognit', 'function', 'whatnot', 'left', 'thing', 'moder', 'sim', 'inde', 'video', 'game', 'good', 'one', 'note', 'good', 'one', 'somewhat', 'subject', 'complet', 'promot', 'death', 'given', 'sim', 'dear', 'enfp', 'favorit', 'video', 'game', 'grow', 'current', 'favorit', 'video', 'game', 'cool', 'appear', 'late', 'sad', 'someon', 'everyon', 'wait', 'thought', 'confid', 'good', 'thing', 'cherish', 'time', 'solitud', 'b', 'c', 'revel', 'within', 'inner', 'world', 'wherea', 'time', 'workin', 'enjoy', 'time', 'worri', 'peopl', 'alway', 'around', 'yo', 'entp', 'ladi', 'complimentari', 'person', 'well', 'hey', 'main', 'social', 'outlet', 'xbox', 'live', 'convers', 'even', 'verbal', 'fatigu', 'quick', 'realli', 'dig', 'part', '1', '46', '2', '50', 'ban', 'thread', 'requir', 'get', 'high', 'backyard', 'roast', 'eat', 'marshmellow', 'backyard', 'convers', 'someth', 'intellectu', 'follow', 'massag', 'kiss', 'ban', 'mani', 'sentenc', 'could', 'think', 'b', 'ban', 'watch', 'movi', 'corner', 'dunc', 'ban', 'health', 'class', 'clear', 'taught', 'noth', 'peer', 'pressur', 'ban', 'whole', 'host', 'reason', '1', 'two', 'babi', 'deer', 'left', 'right', 'munch', 'beetl', 'middl', '2', 'use', 'blood', 'two', 'caveman', 'diari', 'latest', 'happen', 'design', 'cave', 'diari', 'wall', '3', 'see', 'pokemon', 'world', 'infj', 'societi', 'everyon', 'becom', 'optimist', '49142', 'artist', 'artist', 'draw', 'idea', 'count', 'form', 'someth', 'like', 'signatur', 'welcom', 'robot', 'rank', 'person', 'down', 'self', 'esteem', 'cuz', 'avid', 'signatur', 'artist', 'like', 'proud', 'ban', 'take', 'room', 'bed', 'ya', 'got', 'ta', 'learn', 'share', 'roach', 'ban', 'much', 'thunder', 'grumbl', 'kind', 'storm', 'yep', 'ahh', 'old', 'high', 'school', 'music', 'heard', 'age', 'fail', 'public', 'speak', 'class', 'year', 'ago', 'sort', 'learn', 'could', 'better', 'posit', 'big', 'part', 'failur', 'overload', 'like', 'mental', 'confirm', 'intj', 'way', 'move', 'denver', 'area', 'start', 'new', 'life']\n"
     ]
    }
   ],
   "source": [
    "print(f'Output:\\n{Preprocessor(data_copy.posts[0])}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e869ac8efe90cbfac82bf7a18539a20990ac3d9cb539ed36e4bdd738f05377b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
