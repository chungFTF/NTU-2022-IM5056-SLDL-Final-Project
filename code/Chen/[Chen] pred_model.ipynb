{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/chenenying/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/chenenying/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/chenenying/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chenenying/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### import package ###\n",
    "\n",
    "# from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定義前處理function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocessor(text:str, stemmer: str='Snowball', remove_mbti: bool=False) -> list:\n",
    "\t'''\n",
    "\tInput: str\n",
    "\tOutput: list\n",
    "\t\tPreprocessed tokens\n",
    "\tstemmer: str\n",
    "\t\tCan be 'Snowball' or 'Porter'. Default is Snowball.\n",
    "\tremove_mbti: bool\n",
    "\t\tRemove MBTI keywords like INTJ, ENFP, etc. Default is False.(Keep MBTI keywords.)\n",
    "\t'''\n",
    "\t# Cleaning\n",
    "\ttext = re.sub(r'\\|\\|\\|', ' ', text)  # Split by separator\n",
    "\ttext = re.sub(r'http\\S+', ' ', text)  # Replace hyperlink\n",
    "\ttext = re.sub(r\"[A-Za-z]+\\'+\\w+\", ' ', text)  # Handling apostrophe (e.g. you've, there's)\n",
    "\ttext = re.sub('[^0-9a-zA-Z]',' ', text)  # Keep only numbers and alphabets (remove special characters)\n",
    "\ttext = text.lower()\n",
    "\tif remove_mbti == True:\n",
    "\t\ttext = re.sub('intj|intp|entj|entp|infp|enfj|enfp|istj|isfj|estj|esfj|istp|isfp|estp|esfp|infj', '', text)\n",
    "  \t# Tokenization\n",
    "\ttokens = word_tokenize(text)\n",
    "\tfiltered_tokens = [w for w in tokens if not w in stopwords.words('english')]  # Remove stopwords\n",
    "\t# Stemming\n",
    "\tstemmer_ = SnowballStemmer(\"english\")\n",
    "\tif stemmer == 'Porter|porter':\n",
    "\t\tstemmer_ = PorterStemmer()\n",
    "\tif stemmer not in ['Snowball', 'snowball', 'Porter', 'porter']:\n",
    "\t\traise ValueError(\"Please check passed argument: stemmer must be 'Snowball' or 'Porter'\")\n",
    "\tstemmed = [stemmer_.stem(t) for t in filtered_tokens]\n",
    "\t# Lemmatizing\n",
    "\tlemma = WordNetLemmatizer()\n",
    "\t# lemmatized = [lemma.lemmatize(t) for t in stemmed]\n",
    "\tlemmatized = \" \".join([lemma.lemmatize(w) for w in stemmed])   # .join() -> 用空格分開每個字\n",
    "\treturn lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 測試前處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"http://www.youtube.com/watch?v=qsXHcwe3krw|||http://41.media.tumblr.com/tumblr_lfouy03PMA1qa1rooo1_500.jpg|||enfp and intj moments sportly center not top ten plays  https://www.youtube.com/watch?v=uCdfze1etec  prank|||What has been the most life-changing experience in your life?|||The last thing my sportly INFJ friend posted on his facebook: Rest in peace~   http://vimeo.com/22842206|||Hello ENFJ7. It's only natural for a relationship to not be perfection all the time in every moment of existence.\"\n",
    "\n",
    "# stemmer=snowball, remove_MBTI=false\n",
    "txt_snow = Preprocessor(txt)\n",
    "\n",
    "# stemmer = snowball, remove_MBTI = true\n",
    "txt_snow_removenMBTI = Preprocessor(txt, remove_mbti=True)\n",
    "\n",
    "# stemmer = porter, remove_MBTI = false\n",
    "txt_porter = Preprocessor(txt, stemmer='Porter')\n",
    "\n",
    "# stemmer = porter, remove_MBTI = true\n",
    "txt_porter_removeMBTI = Preprocessor(txt, stemmer='porter', remove_mbti=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'enfp intj moment sport center top ten play prank life chang experi life last thing sport infj friend post facebook rest peac hello enfj7 natur relationship perfect time everi moment exist'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_snow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model使用：XGBoost\n",
    "\n",
    "- part 1 : remove_MBTI = True\n",
    "- part 2 : remove_MBTI = False\n",
    "- part 3 : remove_MBTI = True, 4 models\n",
    "- part 4 : remove_MBTI = True, combine of 4 models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 : \n",
    "## training data : snowball stemmer with remove_MBTI = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/Kaggle_MBTI.csv')\n",
    "data_snow_removeMBTI = data.copy()\n",
    "\n",
    "for d in range(len(data)):\n",
    "    post = data.loc[d, 'posts']\n",
    "\n",
    "    txt_snow_removenMBTI = Preprocessor(post, remove_mbti=True)\n",
    "\n",
    "    data_snow_removeMBTI.posts[d] = txt_snow_removenMBTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'moment sportscent top ten play prank life chang experi life repeat today may perc experi immers last thing friend post facebook commit suicid next day rest peac hello 7 sorri hear distress natur relationship perfect time everi moment exist tri figur hard time time growth 84389 84390 welcom stuff game set match prozac wellbrutin least thirti minut move leg mean move sit desk chair weed moder mayb tri edibl healthier altern basic come three item determin type whichev type want would like use given type cognit function whatnot left thing moder sim inde video game good one note good one somewhat subject complet promot death given sim dear favorit video game grow current favorit video game cool appear late sad someon everyon wait thought confid good thing cherish time solitud b c revel within inner world wherea time workin enjoy time worri peopl alway around yo ladi complimentari person well hey main social outlet xbox live convers even verbal fatigu quick realli dig part 1 46 2 50 ban thread requir get high backyard roast eat marshmellow backyard convers someth intellectu follow massag kiss ban mani sentenc could think b ban watch movi corner dunc ban health class clear taught noth peer pressur ban whole host reason 1 two babi deer left right munch beetl middl 2 use blood two caveman diari latest happen design cave diari wall 3 see pokemon world societi everyon becom optimist 49142 artist artist draw idea count form someth like signatur welcom robot rank person down self esteem cuz avid signatur artist like proud ban take room bed ya got ta learn share roach ban much thunder grumbl kind storm yep ahh old high school music heard age fail public speak class year ago sort learn could better posit big part failur overload like mental confirm way move denver area start new life'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_posts = []\n",
    "for i in range(len(data_snow_removeMBTI)):\n",
    "    post = data_snow_removeMBTI.loc[i, 'posts']\n",
    "    list_posts.append(post)\n",
    "list_posts = np.array(list_posts)\n",
    "list_posts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CountVectorizer :\n",
      "\n",
      "Using Tf-idf :\n",
      "Now the dataset size is as below\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8675, 77959)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_posts = []\n",
    "for i in range(len(data_snow_removeMBTI)):\n",
    "    post = data_snow_removeMBTI.loc[i, 'posts']\n",
    "    list_posts.append(post)\n",
    "list_posts = np.array(list_posts)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Vectorizing the database posts to a matrix of token counts for the model\n",
    "cntizer = CountVectorizer() \n",
    "                        \n",
    "# the feature should be made of word n-gram \n",
    "# Learn the vocabulary dictionary and return term-document matrix\n",
    "print(\"Using CountVectorizer :\")\n",
    "X_cnt = cntizer.fit_transform(list_posts)\n",
    "col_name = cntizer.get_feature_names_out()   # 紀錄 sparse matrix 的字分別是哪些字\n",
    "\n",
    "\n",
    "# For the Standardization or Feature Scaling Stage :-\n",
    "# Transform the count matrix to a normalized tf or tf-idf representation\n",
    "tfizer = TfidfTransformer()\n",
    "\n",
    "# Learn the idf vector (fit) and transform a count matrix to a tf-idf representation\n",
    "print(\"\\nUsing Tf-idf :\")\n",
    "\n",
    "print(\"Now the dataset size is as below\")\n",
    "X_tfidf =  tfizer.fit_transform(X_cnt).toarray()\n",
    "X_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.18321351e-01, -2.45834207e-02, -3.84752551e-02, ...,\n",
       "         5.50076640e-03, -4.15293961e-02, -4.41515198e-03],\n",
       "       [ 3.47231658e-01,  1.44933018e-02,  4.14309011e-04, ...,\n",
       "         7.04989659e-03, -3.06592309e-02, -2.20784484e-02],\n",
       "       [ 2.95310602e-01, -2.54562934e-02, -3.38758266e-02, ...,\n",
       "        -2.09563284e-02, -1.46703506e-02,  7.36759977e-03],\n",
       "       ...,\n",
       "       [ 2.94277665e-01, -3.72974752e-02, -1.79301326e-02, ...,\n",
       "        -9.23416395e-03, -1.10198841e-02,  6.83930269e-03],\n",
       "       [ 5.03995256e-01, -2.93056982e-03, -4.57695322e-02, ...,\n",
       "        -2.71498123e-02, -3.37654209e-02, -1.60298056e-02],\n",
       "       [ 4.73248056e-01, -7.81700205e-02, -2.36295542e-02, ...,\n",
       "         2.18301947e-02,  5.85967817e-02,  1.52859661e-02]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=100)\n",
    "result = svd.fit_transform(X_tfidf)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.218321</td>\n",
       "      <td>-0.024583</td>\n",
       "      <td>-0.038475</td>\n",
       "      <td>0.051253</td>\n",
       "      <td>0.052573</td>\n",
       "      <td>-0.098390</td>\n",
       "      <td>-0.012897</td>\n",
       "      <td>-0.012861</td>\n",
       "      <td>0.007571</td>\n",
       "      <td>0.026597</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046387</td>\n",
       "      <td>0.004768</td>\n",
       "      <td>0.035041</td>\n",
       "      <td>-0.022150</td>\n",
       "      <td>-0.033650</td>\n",
       "      <td>-0.001184</td>\n",
       "      <td>0.003872</td>\n",
       "      <td>0.005501</td>\n",
       "      <td>-0.041529</td>\n",
       "      <td>-0.004415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.347232</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>0.002264</td>\n",
       "      <td>0.022911</td>\n",
       "      <td>-0.027197</td>\n",
       "      <td>-0.053938</td>\n",
       "      <td>-0.050517</td>\n",
       "      <td>0.010498</td>\n",
       "      <td>-0.031986</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011009</td>\n",
       "      <td>-0.024145</td>\n",
       "      <td>-0.033613</td>\n",
       "      <td>-0.018406</td>\n",
       "      <td>-0.001637</td>\n",
       "      <td>-0.010336</td>\n",
       "      <td>-0.019465</td>\n",
       "      <td>0.007050</td>\n",
       "      <td>-0.030659</td>\n",
       "      <td>-0.022078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.295311</td>\n",
       "      <td>-0.025456</td>\n",
       "      <td>-0.033876</td>\n",
       "      <td>0.015701</td>\n",
       "      <td>0.017684</td>\n",
       "      <td>-0.048588</td>\n",
       "      <td>-0.011597</td>\n",
       "      <td>-0.027767</td>\n",
       "      <td>0.025643</td>\n",
       "      <td>-0.018619</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039269</td>\n",
       "      <td>-0.038023</td>\n",
       "      <td>0.009555</td>\n",
       "      <td>-0.018155</td>\n",
       "      <td>0.010268</td>\n",
       "      <td>-0.002686</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>-0.020956</td>\n",
       "      <td>-0.014670</td>\n",
       "      <td>0.007368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.365347</td>\n",
       "      <td>0.138809</td>\n",
       "      <td>-0.019489</td>\n",
       "      <td>0.044647</td>\n",
       "      <td>-0.018550</td>\n",
       "      <td>-0.006768</td>\n",
       "      <td>0.001171</td>\n",
       "      <td>0.075980</td>\n",
       "      <td>-0.030990</td>\n",
       "      <td>-0.035420</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012199</td>\n",
       "      <td>-0.019985</td>\n",
       "      <td>0.022097</td>\n",
       "      <td>-0.006371</td>\n",
       "      <td>0.009820</td>\n",
       "      <td>0.013881</td>\n",
       "      <td>-0.020020</td>\n",
       "      <td>-0.027712</td>\n",
       "      <td>0.013648</td>\n",
       "      <td>0.012111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.306189</td>\n",
       "      <td>-0.015107</td>\n",
       "      <td>-0.028018</td>\n",
       "      <td>0.074865</td>\n",
       "      <td>-0.032701</td>\n",
       "      <td>0.027101</td>\n",
       "      <td>0.010016</td>\n",
       "      <td>-0.008506</td>\n",
       "      <td>-0.083858</td>\n",
       "      <td>-0.042293</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026158</td>\n",
       "      <td>-0.009088</td>\n",
       "      <td>-0.017805</td>\n",
       "      <td>0.016174</td>\n",
       "      <td>0.002986</td>\n",
       "      <td>-0.007500</td>\n",
       "      <td>-0.009730</td>\n",
       "      <td>0.018737</td>\n",
       "      <td>0.000721</td>\n",
       "      <td>-0.009504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8670</th>\n",
       "      <td>0.283351</td>\n",
       "      <td>-0.021577</td>\n",
       "      <td>-0.029227</td>\n",
       "      <td>0.041018</td>\n",
       "      <td>0.016443</td>\n",
       "      <td>-0.048257</td>\n",
       "      <td>-0.009345</td>\n",
       "      <td>-0.010493</td>\n",
       "      <td>-0.074849</td>\n",
       "      <td>-0.002368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045288</td>\n",
       "      <td>-0.007694</td>\n",
       "      <td>0.008313</td>\n",
       "      <td>0.001655</td>\n",
       "      <td>-0.002699</td>\n",
       "      <td>-0.021647</td>\n",
       "      <td>0.009463</td>\n",
       "      <td>0.028501</td>\n",
       "      <td>0.010709</td>\n",
       "      <td>0.011473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8671</th>\n",
       "      <td>0.408075</td>\n",
       "      <td>0.009098</td>\n",
       "      <td>-0.022288</td>\n",
       "      <td>-0.009931</td>\n",
       "      <td>0.037114</td>\n",
       "      <td>-0.089582</td>\n",
       "      <td>0.025174</td>\n",
       "      <td>-0.016819</td>\n",
       "      <td>-0.056482</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022417</td>\n",
       "      <td>-0.027183</td>\n",
       "      <td>-0.010989</td>\n",
       "      <td>0.042422</td>\n",
       "      <td>0.025413</td>\n",
       "      <td>0.014192</td>\n",
       "      <td>0.015489</td>\n",
       "      <td>-0.027111</td>\n",
       "      <td>0.004296</td>\n",
       "      <td>-0.021201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8672</th>\n",
       "      <td>0.294278</td>\n",
       "      <td>-0.037297</td>\n",
       "      <td>-0.017930</td>\n",
       "      <td>0.092755</td>\n",
       "      <td>-0.007925</td>\n",
       "      <td>-0.026852</td>\n",
       "      <td>0.017868</td>\n",
       "      <td>-0.005668</td>\n",
       "      <td>-0.018594</td>\n",
       "      <td>-0.076833</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012156</td>\n",
       "      <td>0.003705</td>\n",
       "      <td>0.004381</td>\n",
       "      <td>-0.060154</td>\n",
       "      <td>-0.010956</td>\n",
       "      <td>-0.008312</td>\n",
       "      <td>-0.039472</td>\n",
       "      <td>-0.009234</td>\n",
       "      <td>-0.011020</td>\n",
       "      <td>0.006839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8673</th>\n",
       "      <td>0.503995</td>\n",
       "      <td>-0.002931</td>\n",
       "      <td>-0.045770</td>\n",
       "      <td>-0.008551</td>\n",
       "      <td>-0.016652</td>\n",
       "      <td>0.011997</td>\n",
       "      <td>0.038163</td>\n",
       "      <td>0.026382</td>\n",
       "      <td>0.011663</td>\n",
       "      <td>-0.010993</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028005</td>\n",
       "      <td>0.004002</td>\n",
       "      <td>0.005614</td>\n",
       "      <td>0.030821</td>\n",
       "      <td>-0.028366</td>\n",
       "      <td>-0.038388</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>-0.027150</td>\n",
       "      <td>-0.033765</td>\n",
       "      <td>-0.016030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8674</th>\n",
       "      <td>0.473248</td>\n",
       "      <td>-0.078170</td>\n",
       "      <td>-0.023630</td>\n",
       "      <td>-0.017215</td>\n",
       "      <td>-0.016017</td>\n",
       "      <td>0.021247</td>\n",
       "      <td>0.019835</td>\n",
       "      <td>-0.010453</td>\n",
       "      <td>0.005093</td>\n",
       "      <td>-0.019223</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027847</td>\n",
       "      <td>0.021653</td>\n",
       "      <td>-0.016531</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>0.013093</td>\n",
       "      <td>0.015101</td>\n",
       "      <td>0.034397</td>\n",
       "      <td>0.021830</td>\n",
       "      <td>0.058597</td>\n",
       "      <td>0.015286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8675 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0     0.218321 -0.024583 -0.038475  0.051253  0.052573 -0.098390 -0.012897   \n",
       "1     0.347232  0.014493  0.000414  0.002264  0.022911 -0.027197 -0.053938   \n",
       "2     0.295311 -0.025456 -0.033876  0.015701  0.017684 -0.048588 -0.011597   \n",
       "3     0.365347  0.138809 -0.019489  0.044647 -0.018550 -0.006768  0.001171   \n",
       "4     0.306189 -0.015107 -0.028018  0.074865 -0.032701  0.027101  0.010016   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "8670  0.283351 -0.021577 -0.029227  0.041018  0.016443 -0.048257 -0.009345   \n",
       "8671  0.408075  0.009098 -0.022288 -0.009931  0.037114 -0.089582  0.025174   \n",
       "8672  0.294278 -0.037297 -0.017930  0.092755 -0.007925 -0.026852  0.017868   \n",
       "8673  0.503995 -0.002931 -0.045770 -0.008551 -0.016652  0.011997  0.038163   \n",
       "8674  0.473248 -0.078170 -0.023630 -0.017215 -0.016017  0.021247  0.019835   \n",
       "\n",
       "            7         8         9   ...        90        91        92  \\\n",
       "0    -0.012861  0.007571  0.026597  ...  0.046387  0.004768  0.035041   \n",
       "1    -0.050517  0.010498 -0.031986  ... -0.011009 -0.024145 -0.033613   \n",
       "2    -0.027767  0.025643 -0.018619  ...  0.039269 -0.038023  0.009555   \n",
       "3     0.075980 -0.030990 -0.035420  ... -0.012199 -0.019985  0.022097   \n",
       "4    -0.008506 -0.083858 -0.042293  ... -0.026158 -0.009088 -0.017805   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "8670 -0.010493 -0.074849 -0.002368  ...  0.045288 -0.007694  0.008313   \n",
       "8671 -0.016819 -0.056482  0.000131  ... -0.022417 -0.027183 -0.010989   \n",
       "8672 -0.005668 -0.018594 -0.076833  ... -0.012156  0.003705  0.004381   \n",
       "8673  0.026382  0.011663 -0.010993  ...  0.028005  0.004002  0.005614   \n",
       "8674 -0.010453  0.005093 -0.019223  ...  0.027847  0.021653 -0.016531   \n",
       "\n",
       "            93        94        95        96        97        98        99  \n",
       "0    -0.022150 -0.033650 -0.001184  0.003872  0.005501 -0.041529 -0.004415  \n",
       "1    -0.018406 -0.001637 -0.010336 -0.019465  0.007050 -0.030659 -0.022078  \n",
       "2    -0.018155  0.010268 -0.002686  0.019000 -0.020956 -0.014670  0.007368  \n",
       "3    -0.006371  0.009820  0.013881 -0.020020 -0.027712  0.013648  0.012111  \n",
       "4     0.016174  0.002986 -0.007500 -0.009730  0.018737  0.000721 -0.009504  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "8670  0.001655 -0.002699 -0.021647  0.009463  0.028501  0.010709  0.011473  \n",
       "8671  0.042422  0.025413  0.014192  0.015489 -0.027111  0.004296 -0.021201  \n",
       "8672 -0.060154 -0.010956 -0.008312 -0.039472 -0.009234 -0.011020  0.006839  \n",
       "8673  0.030821 -0.028366 -0.038388  0.000297 -0.027150 -0.033765 -0.016030  \n",
       "8674  0.009000  0.013093  0.015101  0.034397  0.021830  0.058597  0.015286  \n",
       "\n",
       "[8675 rows x 100 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf = pd.DataFrame(result)\n",
    "df_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenenying/opt/anaconda3/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 34.72%\n"
     ]
    }
   ],
   "source": [
    "label = data.loc[:,['type']]\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "Y = LabelEncoder().fit_transform(label) \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_tfidf, Y, test_size=0.33, random_state=42)\n",
    "eval_set = [(X_test, y_test)]\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "#XG boost Classifier\n",
    "param = {}\n",
    "\n",
    "param['n_estimators'] = 100\n",
    "param['max_depth'] = 2\n",
    "param['learning_rate'] = 0.2\n",
    "\n",
    "xgb = XGBClassifier(**param)\n",
    "xgb_model = xgb.fit(X_train,y_train)\n",
    "\n",
    "Y_pred = xgb_model.predict(X_test)\n",
    "predictions = [round(value) for value in Y_pred]\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grid search for XGboost(remove_MBTI = True, Model = XGboost, data = raw data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimator = 100, learning_rate = 0.2, best accuracy = 0.3471882640586797 \n",
      "n_estimator = 200, learning_rate = 0.18, best accuracy = 0.3471882640586797 \n",
      "n_estimator = 300, learning_rate = 0.18, best accuracy = 0.34963325183374083 \n",
      "n_estimator = 400, learning_rate = 0.2, best accuracy = 0.35103038770520434 \n",
      "n_estimator = 500, learning_rate = 0.12, best accuracy = 0.3517289556409361 \n",
      "n_estimator = 600, learning_rate = 0.15, best accuracy = 0.3499825358016067 \n",
      "n_estimator = 700, learning_rate = 0.12, best accuracy = 0.354872511351729 \n",
      "n_estimator = 800, learning_rate = 0.12, best accuracy = 0.35207823960880197 \n",
      "n_estimator = 900, learning_rate = 0.1, best accuracy = 0.3545232273838631 \n",
      "n_estimator = 1000, learning_rate = 0.1, best accuracy = 0.3545232273838631 \n",
      "Result : best n_estimator = 700, best learning_rate = 0.12, best accuracy = 0.354872511351729 \n"
     ]
    }
   ],
   "source": [
    "n_estimators = [int(x) for x in np.linspace(start=100, stop=1000, num=10)]\n",
    "learning_rate=[round(float(x),2) for x in np.linspace(start=0.1, stop=0.2, num=5)]\n",
    "\n",
    "best_nest = 0\n",
    "best_lr = 0\n",
    "best_acc=0\n",
    "\n",
    "for nest in n_estimators:\n",
    "    local_acc = 0\n",
    "    local_lr = 0\n",
    "    for lr in learning_rate:\n",
    "        param = {}\n",
    "        param['n_estimators'] = nest\n",
    "        param['max_depth'] = 2\n",
    "        param['learning_rate'] = lr\n",
    "\n",
    "        xgb = XGBClassifier(**param)\n",
    "        xgb_model = xgb.fit(X_train,y_train)\n",
    "\n",
    "        Y_pred = xgb_model.predict(X_test)\n",
    "        predictions = [round(value) for value in Y_pred]\n",
    "\n",
    "        # evaluate predictions\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "        if accuracy > local_acc:\n",
    "            local_acc = accuracy\n",
    "            local_lr = lr\n",
    "\n",
    "        if accuracy > best_acc:\n",
    "            best_acc = accuracy\n",
    "            best_nest = nest\n",
    "            best_lr = lr\n",
    "    \n",
    "    print(f'n_estimator = {nest}, learning_rate = {local_lr}, best accuracy = {local_acc} ')\n",
    "\n",
    "print(f'Result : best n_estimator = {best_nest}, best learning_rate = {best_lr}, best accuracy = {best_acc} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result\n",
    "remove_MBTI = True, data = raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenenying/opt/anaconda3/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 36.19%\n",
      "Micro Precision: 36.19%\n",
      "Macro Precision: 23.16%\n",
      "Micro Recall: 36.19%\n",
      "Macro Recall: 16.78%\n",
      "Micro F1score: 36.19%\n",
      "Macro F1score: 16.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenenying/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=100)\n",
    "result = svd.fit_transform(X_tfidf)\n",
    "df_tfidf = pd.DataFrame(result)    # X before train-test-split\n",
    "\n",
    "label = data.loc[:,['type']]\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "Y = LabelEncoder().fit_transform(label)   # Y before train-test-split\n",
    "\n",
    "# train-test-split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_tfidf, Y, test_size=0.33, random_state=42)\n",
    "\n",
    "#XG boost Classifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "param = {}\n",
    "\n",
    "param['n_estimators'] = 700 # after tunning\n",
    "param['max_depth'] = 2\n",
    "param['learning_rate'] = 0.12 # after tunning\n",
    "\n",
    "xgb = XGBClassifier(**param)\n",
    "xgb_model = xgb.fit(X_train,y_train)\n",
    "\n",
    "Y_pred = xgb_model.predict(X_test)\n",
    "predictions = [round(value) for value in Y_pred]\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "precision_mi = precision_score(y_test, predictions, average='micro')\n",
    "print(\"Micro Precision: %.2f%%\" % (precision_mi * 100.0))\n",
    "precision_ma = precision_score(y_test, predictions, average='macro')\n",
    "print(\"Macro Precision: %.2f%%\" % (precision_ma * 100.0))\n",
    "\n",
    "recall_mi = recall_score(y_test, predictions, average='micro')\n",
    "print(\"Micro Recall: %.2f%%\" % (recall_mi * 100.0))\n",
    "recall_ma = recall_score(y_test, predictions, average='macro')\n",
    "print(\"Macro Recall: %.2f%%\" % (recall_ma * 100.0))\n",
    "\n",
    "f1score_mi = f1_score(y_test, predictions, average='micro')\n",
    "print(\"Micro F1score: %.2f%%\" % (f1score_mi * 100.0))\n",
    "f1score_ma = f1_score(y_test, predictions, average='macro')\n",
    "print(\"Macro F1score: %.2f%%\" % (f1score_ma * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result for XGboost after grid search : \n",
    "remove_MBTI = True, data = SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenenying/opt/anaconda3/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimator = 100, learning_rate = 0.2, best accuracy = 0.26720223541739435 \n",
      "n_estimator = 200, learning_rate = 0.2, best accuracy = 0.29269996507160323 \n",
      "n_estimator = 300, learning_rate = 0.18, best accuracy = 0.3028292001397136 \n",
      "n_estimator = 400, learning_rate = 0.2, best accuracy = 0.3108627314006287 \n",
      "n_estimator = 500, learning_rate = 0.18, best accuracy = 0.31610199091861685 \n",
      "n_estimator = 600, learning_rate = 0.18, best accuracy = 0.32238910234020257 \n",
      "n_estimator = 700, learning_rate = 0.2, best accuracy = 0.3293747816975201 \n",
      "n_estimator = 800, learning_rate = 0.18, best accuracy = 0.3346140412155082 \n",
      "n_estimator = 900, learning_rate = 0.18, best accuracy = 0.33391547327977644 \n",
      "n_estimator = 1000, learning_rate = 0.2, best accuracy = 0.3363604610548376 \n",
      "Result : best n_estimator = 1000, best learning_rate = 0.2, best accuracy = 0.3363604610548376 \n",
      "-----------------------------------------------------------\n",
      "Result for XGboost after grid search (remove_MBTI = True, Model = XGboost, data = SMOTE)\n",
      "Accuracy: 33.64%\n",
      "Micro Precision: 33.64%\n",
      "Macro Precision: 19.14%\n",
      "Micro Recall: 33.64%\n",
      "Macro Recall: 18.17%\n",
      "Micro F1score: 33.64%\n",
      "Macro F1score: 18.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenenying/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=100)\n",
    "result = svd.fit_transform(X_tfidf)\n",
    "df_tfidf = pd.DataFrame(result)    # X before train-test-split\n",
    "\n",
    "label = data.loc[:,['type']]\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "Y = LabelEncoder().fit_transform(label)   # Y before train-test-split\n",
    "\n",
    "# train-test-split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_tfidf, Y, test_size=0.33, random_state=42)\n",
    "\n",
    "# SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "X_re, y_re = SMOTE(random_state=42).fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "# grid search\n",
    "n_estimators = [int(x) for x in np.linspace(start=100, stop=1000, num=10)]\n",
    "learning_rate=[round(float(x),2) for x in np.linspace(start=0.1, stop=0.2, num=5)]\n",
    "\n",
    "best_nest = 0\n",
    "best_lr = 0\n",
    "best_acc=0\n",
    "\n",
    "for nest in n_estimators:\n",
    "    local_acc = 0\n",
    "    local_lr = 0\n",
    "    for lr in learning_rate:\n",
    "        param = {}\n",
    "        param['n_estimators'] = nest\n",
    "        param['max_depth'] = 2\n",
    "        param['learning_rate'] = lr\n",
    "\n",
    "        xgb = XGBClassifier(**param)\n",
    "        xgb_model = xgb.fit(X_re,y_re)\n",
    "\n",
    "        Y_pred = xgb_model.predict(X_test)\n",
    "        predictions = [round(value) for value in Y_pred]\n",
    "\n",
    "        # evaluate predictions\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "        if accuracy > local_acc:\n",
    "            local_acc = accuracy\n",
    "            local_lr = lr\n",
    "\n",
    "        if accuracy > best_acc:\n",
    "            best_acc = accuracy\n",
    "            best_nest = nest\n",
    "            best_lr = lr\n",
    "    \n",
    "    print(f'n_estimator = {nest}, learning_rate = {local_lr}, best accuracy = {local_acc} ')\n",
    "\n",
    "print(f'Result : best n_estimator = {best_nest}, best learning_rate = {best_lr}, best accuracy = {best_acc} ')\n",
    "print('-----------------------------------------------------------')\n",
    "\n",
    "\n",
    "\n",
    "#XG boost Classifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "param = {}\n",
    "\n",
    "param['n_estimators'] = best_nest # after tunning\n",
    "param['max_depth'] = 2\n",
    "param['learning_rate'] = best_lr # after tunning\n",
    "\n",
    "xgb = XGBClassifier(**param)\n",
    "xgb_model = xgb.fit(X_re, y_re)\n",
    "\n",
    "Y_pred = xgb_model.predict(X_test)\n",
    "predictions = [round(value) for value in Y_pred]\n",
    "\n",
    "print('Result for XGboost after grid search (remove_MBTI = True, Model = XGboost, data = SMOTE)')\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "precision_mi = precision_score(y_test, predictions, average='micro')\n",
    "print(\"Micro Precision: %.2f%%\" % (precision_mi * 100.0))\n",
    "precision_ma = precision_score(y_test, predictions, average='macro')\n",
    "print(\"Macro Precision: %.2f%%\" % (precision_ma * 100.0))\n",
    "\n",
    "recall_mi = recall_score(y_test, predictions, average='micro')\n",
    "print(\"Micro Recall: %.2f%%\" % (recall_mi * 100.0))\n",
    "recall_ma = recall_score(y_test, predictions, average='macro')\n",
    "print(\"Macro Recall: %.2f%%\" % (recall_ma * 100.0))\n",
    "\n",
    "f1score_mi = f1_score(y_test, predictions, average='micro')\n",
    "print(\"Micro F1score: %.2f%%\" % (f1score_mi * 100.0))\n",
    "f1score_ma = f1_score(y_test, predictions, average='macro')\n",
    "print(\"Macro F1score: %.2f%%\" % (f1score_ma * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 : \n",
    "## training data : snowball stemmer with remove_MBTI = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/Kaggle_MBTI.csv')\n",
    "data_snow_preserveMBTI = data.copy()\n",
    "\n",
    "for d in range(len(data)):\n",
    "    post = data.loc[d, 'posts']\n",
    "\n",
    "    txt_snow_preserveMBTI = Preprocessor(post, remove_mbti=False)\n",
    "\n",
    "    data_snow_preserveMBTI.posts[d] = txt_snow_preserveMBTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CountVectorizer :\n",
      "\n",
      "Using Tf-idf :\n",
      "Now the dataset size is as below\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8675, 78195)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_posts_preserveMBTI = []\n",
    "for i in range(len(data_snow_preserveMBTI)):\n",
    "    post = data_snow_preserveMBTI.loc[i, 'posts']\n",
    "    list_posts_preserveMBTI.append(post)\n",
    "list_posts_preserveMBTI = np.array(list_posts_preserveMBTI)\n",
    "list_posts_preserveMBTI[0]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Vectorizing the database posts to a matrix of token counts for the model\n",
    "cntizer = CountVectorizer() \n",
    "                        \n",
    "# the feature should be made of word n-gram \n",
    "# Learn the vocabulary dictionary and return term-document matrix\n",
    "print(\"Using CountVectorizer :\")\n",
    "X_cnt = cntizer.fit_transform(list_posts_preserveMBTI)\n",
    "col_name = cntizer.get_feature_names_out()   # 紀錄 sparse matrix 的字分別是哪些字\n",
    "\n",
    "\n",
    "# For the Standardization or Feature Scaling Stage :-\n",
    "# Transform the count matrix to a normalized tf or tf-idf representation\n",
    "tfizer = TfidfTransformer()\n",
    "\n",
    "# Learn the idf vector (fit) and transform a count matrix to a tf-idf representation\n",
    "print(\"\\nUsing Tf-idf :\")\n",
    "\n",
    "print(\"Now the dataset size is as below\")\n",
    "X_tfidf_preserveMBTI =  tfizer.fit_transform(X_cnt).toarray()\n",
    "X_tfidf_preserveMBTI.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.23610681e-01, -2.02310166e-02, -3.40873630e-02, ...,\n",
       "         4.18226318e-03, -1.48110021e-02,  4.71118238e-02],\n",
       "       [ 3.54135836e-01,  3.75235553e-02, -1.80221282e-04, ...,\n",
       "        -1.69594789e-02, -1.31130966e-02, -2.45278845e-02],\n",
       "       [ 2.94517923e-01, -4.25355192e-02, -4.69807402e-02, ...,\n",
       "         2.55843178e-05, -3.21370399e-02, -5.93011647e-03],\n",
       "       ...,\n",
       "       [ 2.95972693e-01, -4.04896288e-02, -7.56407070e-02, ...,\n",
       "        -3.01270711e-02, -4.87913584e-03, -1.94552880e-02],\n",
       "       [ 5.03399731e-01, -2.01640743e-02, -1.85485433e-02, ...,\n",
       "        -7.63944536e-03, -3.04604701e-03, -5.71329570e-03],\n",
       "       [ 4.64898298e-01, -1.16113478e-01, -2.54154557e-02, ...,\n",
       "         4.92052934e-03,  8.16109107e-03,  1.90547896e-02]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=100)\n",
    "result = svd.fit_transform(X_tfidf_preserveMBTI)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.223611</td>\n",
       "      <td>-0.020231</td>\n",
       "      <td>-0.034087</td>\n",
       "      <td>0.000962</td>\n",
       "      <td>0.063015</td>\n",
       "      <td>-0.073717</td>\n",
       "      <td>0.057205</td>\n",
       "      <td>0.017520</td>\n",
       "      <td>0.047126</td>\n",
       "      <td>-0.010016</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025138</td>\n",
       "      <td>0.000732</td>\n",
       "      <td>0.036692</td>\n",
       "      <td>0.014167</td>\n",
       "      <td>0.015217</td>\n",
       "      <td>0.018207</td>\n",
       "      <td>0.021154</td>\n",
       "      <td>0.004182</td>\n",
       "      <td>-0.014811</td>\n",
       "      <td>0.047112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.354136</td>\n",
       "      <td>0.037524</td>\n",
       "      <td>-0.000180</td>\n",
       "      <td>0.010968</td>\n",
       "      <td>0.036568</td>\n",
       "      <td>-0.070150</td>\n",
       "      <td>-0.056896</td>\n",
       "      <td>-0.022549</td>\n",
       "      <td>-0.010898</td>\n",
       "      <td>0.019569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012253</td>\n",
       "      <td>0.026911</td>\n",
       "      <td>0.007903</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>-0.001629</td>\n",
       "      <td>-0.011122</td>\n",
       "      <td>-0.028776</td>\n",
       "      <td>-0.016959</td>\n",
       "      <td>-0.013113</td>\n",
       "      <td>-0.024528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.294518</td>\n",
       "      <td>-0.042536</td>\n",
       "      <td>-0.046981</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.018094</td>\n",
       "      <td>-0.029436</td>\n",
       "      <td>0.040829</td>\n",
       "      <td>-0.010940</td>\n",
       "      <td>0.002424</td>\n",
       "      <td>-0.018426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022780</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>-0.031226</td>\n",
       "      <td>-0.004326</td>\n",
       "      <td>0.006249</td>\n",
       "      <td>-0.020192</td>\n",
       "      <td>0.021526</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>-0.032137</td>\n",
       "      <td>-0.005930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.370912</td>\n",
       "      <td>0.117969</td>\n",
       "      <td>-0.071086</td>\n",
       "      <td>0.034069</td>\n",
       "      <td>-0.019328</td>\n",
       "      <td>0.017335</td>\n",
       "      <td>0.025102</td>\n",
       "      <td>-0.007058</td>\n",
       "      <td>0.021839</td>\n",
       "      <td>0.045205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005520</td>\n",
       "      <td>0.009013</td>\n",
       "      <td>-0.033306</td>\n",
       "      <td>-0.012315</td>\n",
       "      <td>-0.036751</td>\n",
       "      <td>-0.019653</td>\n",
       "      <td>0.004027</td>\n",
       "      <td>0.003232</td>\n",
       "      <td>-0.011458</td>\n",
       "      <td>0.011685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.303496</td>\n",
       "      <td>-0.033966</td>\n",
       "      <td>-0.067459</td>\n",
       "      <td>0.052238</td>\n",
       "      <td>-0.015558</td>\n",
       "      <td>0.000597</td>\n",
       "      <td>-0.048641</td>\n",
       "      <td>0.006804</td>\n",
       "      <td>-0.028214</td>\n",
       "      <td>0.025108</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023257</td>\n",
       "      <td>-0.004382</td>\n",
       "      <td>0.034981</td>\n",
       "      <td>-0.035670</td>\n",
       "      <td>0.012652</td>\n",
       "      <td>0.001482</td>\n",
       "      <td>-0.013012</td>\n",
       "      <td>0.017438</td>\n",
       "      <td>-0.032007</td>\n",
       "      <td>0.017551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8670</th>\n",
       "      <td>0.279177</td>\n",
       "      <td>-0.049432</td>\n",
       "      <td>-0.052027</td>\n",
       "      <td>0.021983</td>\n",
       "      <td>0.021391</td>\n",
       "      <td>-0.031876</td>\n",
       "      <td>0.036212</td>\n",
       "      <td>-0.031795</td>\n",
       "      <td>-0.038034</td>\n",
       "      <td>0.009174</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008241</td>\n",
       "      <td>-0.008045</td>\n",
       "      <td>0.052462</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.024739</td>\n",
       "      <td>0.003352</td>\n",
       "      <td>0.019685</td>\n",
       "      <td>0.005127</td>\n",
       "      <td>-0.016426</td>\n",
       "      <td>-0.013010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8671</th>\n",
       "      <td>0.404503</td>\n",
       "      <td>-0.003132</td>\n",
       "      <td>0.007871</td>\n",
       "      <td>-0.026125</td>\n",
       "      <td>0.039692</td>\n",
       "      <td>-0.080074</td>\n",
       "      <td>0.060411</td>\n",
       "      <td>0.016189</td>\n",
       "      <td>-0.055238</td>\n",
       "      <td>0.053366</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032098</td>\n",
       "      <td>-0.031473</td>\n",
       "      <td>-0.024878</td>\n",
       "      <td>0.030667</td>\n",
       "      <td>0.015289</td>\n",
       "      <td>-0.003076</td>\n",
       "      <td>-0.024281</td>\n",
       "      <td>0.060890</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.015073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8672</th>\n",
       "      <td>0.295973</td>\n",
       "      <td>-0.040490</td>\n",
       "      <td>-0.075641</td>\n",
       "      <td>0.080334</td>\n",
       "      <td>0.011291</td>\n",
       "      <td>-0.043678</td>\n",
       "      <td>-0.014257</td>\n",
       "      <td>0.009159</td>\n",
       "      <td>-0.024012</td>\n",
       "      <td>-0.036531</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033448</td>\n",
       "      <td>0.009340</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>-0.011008</td>\n",
       "      <td>-0.004893</td>\n",
       "      <td>0.000511</td>\n",
       "      <td>-0.009006</td>\n",
       "      <td>-0.030127</td>\n",
       "      <td>-0.004879</td>\n",
       "      <td>-0.019455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8673</th>\n",
       "      <td>0.503400</td>\n",
       "      <td>-0.020164</td>\n",
       "      <td>-0.018549</td>\n",
       "      <td>-0.045825</td>\n",
       "      <td>-0.024716</td>\n",
       "      <td>0.029856</td>\n",
       "      <td>0.017269</td>\n",
       "      <td>0.004324</td>\n",
       "      <td>-0.000342</td>\n",
       "      <td>-0.019251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030555</td>\n",
       "      <td>0.044189</td>\n",
       "      <td>0.030708</td>\n",
       "      <td>-0.025583</td>\n",
       "      <td>-0.054790</td>\n",
       "      <td>0.048147</td>\n",
       "      <td>-0.000105</td>\n",
       "      <td>-0.007639</td>\n",
       "      <td>-0.003046</td>\n",
       "      <td>-0.005713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8674</th>\n",
       "      <td>0.464898</td>\n",
       "      <td>-0.116113</td>\n",
       "      <td>-0.025415</td>\n",
       "      <td>-0.018927</td>\n",
       "      <td>-0.029625</td>\n",
       "      <td>0.053899</td>\n",
       "      <td>0.037052</td>\n",
       "      <td>-0.015880</td>\n",
       "      <td>-0.024198</td>\n",
       "      <td>0.019661</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009672</td>\n",
       "      <td>0.011033</td>\n",
       "      <td>0.022527</td>\n",
       "      <td>-0.007289</td>\n",
       "      <td>0.008928</td>\n",
       "      <td>0.017222</td>\n",
       "      <td>0.007545</td>\n",
       "      <td>0.004921</td>\n",
       "      <td>0.008161</td>\n",
       "      <td>0.019055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8675 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0     0.223611 -0.020231 -0.034087  0.000962  0.063015 -0.073717  0.057205   \n",
       "1     0.354136  0.037524 -0.000180  0.010968  0.036568 -0.070150 -0.056896   \n",
       "2     0.294518 -0.042536 -0.046981  0.000538  0.018094 -0.029436  0.040829   \n",
       "3     0.370912  0.117969 -0.071086  0.034069 -0.019328  0.017335  0.025102   \n",
       "4     0.303496 -0.033966 -0.067459  0.052238 -0.015558  0.000597 -0.048641   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "8670  0.279177 -0.049432 -0.052027  0.021983  0.021391 -0.031876  0.036212   \n",
       "8671  0.404503 -0.003132  0.007871 -0.026125  0.039692 -0.080074  0.060411   \n",
       "8672  0.295973 -0.040490 -0.075641  0.080334  0.011291 -0.043678 -0.014257   \n",
       "8673  0.503400 -0.020164 -0.018549 -0.045825 -0.024716  0.029856  0.017269   \n",
       "8674  0.464898 -0.116113 -0.025415 -0.018927 -0.029625  0.053899  0.037052   \n",
       "\n",
       "            7         8         9   ...        90        91        92  \\\n",
       "0     0.017520  0.047126 -0.010016  ... -0.025138  0.000732  0.036692   \n",
       "1    -0.022549 -0.010898  0.019569  ...  0.012253  0.026911  0.007903   \n",
       "2    -0.010940  0.002424 -0.018426  ... -0.022780  0.000692 -0.031226   \n",
       "3    -0.007058  0.021839  0.045205  ...  0.005520  0.009013 -0.033306   \n",
       "4     0.006804 -0.028214  0.025108  ... -0.023257 -0.004382  0.034981   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "8670 -0.031795 -0.038034  0.009174  ... -0.008241 -0.008045  0.052462   \n",
       "8671  0.016189 -0.055238  0.053366  ... -0.032098 -0.031473 -0.024878   \n",
       "8672  0.009159 -0.024012 -0.036531  ... -0.033448  0.009340  0.007724   \n",
       "8673  0.004324 -0.000342 -0.019251  ...  0.030555  0.044189  0.030708   \n",
       "8674 -0.015880 -0.024198  0.019661  ... -0.009672  0.011033  0.022527   \n",
       "\n",
       "            93        94        95        96        97        98        99  \n",
       "0     0.014167  0.015217  0.018207  0.021154  0.004182 -0.014811  0.047112  \n",
       "1     0.025391 -0.001629 -0.011122 -0.028776 -0.016959 -0.013113 -0.024528  \n",
       "2    -0.004326  0.006249 -0.020192  0.021526  0.000026 -0.032137 -0.005930  \n",
       "3    -0.012315 -0.036751 -0.019653  0.004027  0.003232 -0.011458  0.011685  \n",
       "4    -0.035670  0.012652  0.001482 -0.013012  0.017438 -0.032007  0.017551  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "8670  0.000162  0.024739  0.003352  0.019685  0.005127 -0.016426 -0.013010  \n",
       "8671  0.030667  0.015289 -0.003076 -0.024281  0.060890  0.000300  0.015073  \n",
       "8672 -0.011008 -0.004893  0.000511 -0.009006 -0.030127 -0.004879 -0.019455  \n",
       "8673 -0.025583 -0.054790  0.048147 -0.000105 -0.007639 -0.003046 -0.005713  \n",
       "8674 -0.007289  0.008928  0.017222  0.007545  0.004921  0.008161  0.019055  \n",
       "\n",
       "[8675 rows x 100 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf_preserveMBTI = pd.DataFrame(result)\n",
    "df_tfidf_preserveMBTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 65.07%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_tfidf_preserveMBTI, Y, test_size=0.33, random_state=42)\n",
    "eval_set = [(X_test, y_test)]\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "#XG boost Classifier\n",
    "param = {}\n",
    "\n",
    "param['n_estimators'] = 200\n",
    "param['max_depth'] = 2\n",
    "param['learning_rate'] = 0.2\n",
    "\n",
    "xgb = XGBClassifier(**param)\n",
    "xgb_model = xgb.fit(X_train,y_train)\n",
    "\n",
    "Y_pred = xgb_model.predict(X_test)\n",
    "predictions = [round(value) for value in Y_pred]\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result for XGboost after grid search : \n",
    "remove_MBTI = False, data = raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenenying/opt/anaconda3/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimator = 100, learning_rate = 0.15, best accuracy = 0.650716032134125 \n",
      "n_estimator = 200, learning_rate = 0.2, best accuracy = 0.656653859587845 \n",
      "n_estimator = 300, learning_rate = 0.15, best accuracy = 0.6559552916521132 \n",
      "n_estimator = 400, learning_rate = 0.15, best accuracy = 0.6556060076842473 \n",
      "n_estimator = 500, learning_rate = 0.2, best accuracy = 0.6570031435557108 \n",
      "n_estimator = 600, learning_rate = 0.12, best accuracy = 0.6584002794271743 \n",
      "n_estimator = 700, learning_rate = 0.18, best accuracy = 0.6604959832343695 \n",
      "n_estimator = 800, learning_rate = 0.12, best accuracy = 0.6587495633950402 \n",
      "n_estimator = 900, learning_rate = 0.18, best accuracy = 0.6577017114914425 \n",
      "n_estimator = 1000, learning_rate = 0.18, best accuracy = 0.6580509954593085 \n",
      "Result : best n_estimator = 700, best learning_rate = 0.18, best accuracy = 0.6604959832343695 \n",
      "---------------------------------------------------------------\n",
      "Result for XGboost after grid search (remove_MBTI = False, Model = XGboost, data = raw data)\n",
      "Accuracy: 66.05%\n",
      "Micro Precision: 66.05%\n",
      "Macro Precision: 64.76%\n",
      "Micro Recall: 66.05%\n",
      "Macro Recall: 45.72%\n",
      "Micro F1score: 66.05%\n",
      "Macro F1score: 49.93%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=100)\n",
    "result = svd.fit_transform(X_tfidf_preserveMBTI)\n",
    "df_tfidf_preserveMBTI = pd.DataFrame(result)    # X before train-test-split\n",
    "\n",
    "label = data.loc[:,['type']]\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "Y = LabelEncoder().fit_transform(label) \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_tfidf_preserveMBTI, Y, test_size=0.33, random_state=42)\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start=100, stop=1000, num=10)]\n",
    "learning_rate=[round(float(x),2) for x in np.linspace(start=0.1, stop=0.2, num=5)]\n",
    "\n",
    "best_nest = 0\n",
    "best_lr = 0\n",
    "best_acc=0\n",
    "\n",
    "for nest in n_estimators:\n",
    "    local_acc = 0\n",
    "    local_lr = 0\n",
    "    for lr in learning_rate:\n",
    "        param = {}\n",
    "        param['n_estimators'] = nest\n",
    "        param['max_depth'] = 2\n",
    "        param['learning_rate'] = lr\n",
    "\n",
    "        xgb = XGBClassifier(**param)\n",
    "        xgb_model = xgb.fit(X_train,y_train)\n",
    "\n",
    "        Y_pred = xgb_model.predict(X_test)\n",
    "        predictions = [round(value) for value in Y_pred]\n",
    "\n",
    "        # evaluate predictions\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "        if accuracy > local_acc:\n",
    "            local_acc = accuracy\n",
    "            local_lr = lr\n",
    "\n",
    "        if accuracy > best_acc:\n",
    "            best_acc = accuracy\n",
    "            best_nest = nest\n",
    "            best_lr = lr\n",
    "    \n",
    "    print(f'n_estimator = {nest}, learning_rate = {local_lr}, best accuracy = {local_acc} ')\n",
    "\n",
    "print(f'Result : best n_estimator = {best_nest}, best learning_rate = {best_lr}, best accuracy = {best_acc} ')\n",
    "print('---------------------------------------------------------------')\n",
    "\n",
    "\n",
    "\n",
    "param = {}\n",
    "\n",
    "param['n_estimators'] = best_nest # after tunning\n",
    "param['max_depth'] = 2\n",
    "param['learning_rate'] = best_lr # after tunning\n",
    "\n",
    "xgb = XGBClassifier(**param)\n",
    "xgb_model = xgb.fit(X_train,y_train)\n",
    "\n",
    "Y_pred = xgb_model.predict(X_test)\n",
    "predictions = [round(value) for value in Y_pred]\n",
    "\n",
    "print('Result for XGboost after grid search (remove_MBTI = False, Model = XGboost, data = raw data)')\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "precision_mi = precision_score(y_test, predictions, average='micro')\n",
    "print(\"Micro Precision: %.2f%%\" % (precision_mi * 100.0))\n",
    "precision_ma = precision_score(y_test, predictions, average='macro')\n",
    "print(\"Macro Precision: %.2f%%\" % (precision_ma * 100.0))\n",
    "\n",
    "recall_mi = recall_score(y_test, predictions, average='micro')\n",
    "print(\"Micro Recall: %.2f%%\" % (recall_mi * 100.0))\n",
    "recall_ma = recall_score(y_test, predictions, average='macro')\n",
    "print(\"Macro Recall: %.2f%%\" % (recall_ma * 100.0))\n",
    "\n",
    "f1score_mi = f1_score(y_test, predictions, average='micro')\n",
    "print(\"Micro F1score: %.2f%%\" % (f1score_mi * 100.0))\n",
    "f1score_ma = f1_score(y_test, predictions, average='macro')\n",
    "print(\"Macro F1score: %.2f%%\" % (f1score_ma * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result for XGboost after grid search : \n",
    "remove_MBTI = False, data = SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenenying/opt/anaconda3/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimator = 100, learning_rate = 0.2, best accuracy = 0.6192804750261963 \n",
      "n_estimator = 200, learning_rate = 0.18, best accuracy = 0.6398882291302829 \n",
      "n_estimator = 300, learning_rate = 0.2, best accuracy = 0.6465246245197346 \n",
      "n_estimator = 400, learning_rate = 0.15, best accuracy = 0.6545581557806497 \n",
      "n_estimator = 500, learning_rate = 0.2, best accuracy = 0.6531610199091862 \n",
      "n_estimator = 600, learning_rate = 0.18, best accuracy = 0.6521131680055885 \n",
      "n_estimator = 700, learning_rate = 0.2, best accuracy = 0.6521131680055885 \n",
      "n_estimator = 800, learning_rate = 0.12, best accuracy = 0.650716032134125 \n",
      "n_estimator = 900, learning_rate = 0.1, best accuracy = 0.6503667481662592 \n",
      "n_estimator = 1000, learning_rate = 0.1, best accuracy = 0.6493188962626616 \n",
      "Result : best n_estimator = 400, best learning_rate = 0.15, best accuracy = 0.6545581557806497 \n",
      "---------------------------------------------------------------\n",
      "Result for XGboost after grid search (remove_MBTI = False, Model = XGboost, data = SMOTE)\n",
      "Accuracy: 65.46%\n",
      "Micro Precision: 65.46%\n",
      "Macro Precision: 54.44%\n",
      "Micro Recall: 65.46%\n",
      "Macro Recall: 52.03%\n",
      "Micro F1score: 65.46%\n",
      "Macro F1score: 52.52%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=100)\n",
    "result = svd.fit_transform(X_tfidf_preserveMBTI)\n",
    "df_tfidf_preserveMBTI = pd.DataFrame(result)    # X before train-test-split\n",
    "\n",
    "label = data.loc[:,['type']]\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "Y = LabelEncoder().fit_transform(label)   # Y before train-test-split\n",
    "\n",
    "# train-test-split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_tfidf_preserveMBTI, Y, test_size=0.33, random_state=42)\n",
    "\n",
    "# SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "X_re, y_re = SMOTE(random_state=42).fit_resample(X_train, y_train)\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start=100, stop=1000, num=10)]\n",
    "learning_rate=[round(float(x),2) for x in np.linspace(start=0.1, stop=0.2, num=5)]\n",
    "\n",
    "best_nest = 0\n",
    "best_lr = 0\n",
    "best_acc=0\n",
    "\n",
    "for nest in n_estimators:\n",
    "    local_acc = 0\n",
    "    local_lr = 0\n",
    "    for lr in learning_rate:\n",
    "        param = {}\n",
    "        param['n_estimators'] = nest\n",
    "        param['max_depth'] = 2\n",
    "        param['learning_rate'] = lr\n",
    "\n",
    "        xgb = XGBClassifier(**param)\n",
    "        xgb_model = xgb.fit(X_re,y_re)\n",
    "\n",
    "        Y_pred = xgb_model.predict(X_test)\n",
    "        predictions = [round(value) for value in Y_pred]\n",
    "\n",
    "        # evaluate predictions\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "        if accuracy > local_acc:\n",
    "            local_acc = accuracy\n",
    "            local_lr = lr\n",
    "\n",
    "        if accuracy > best_acc:\n",
    "            best_acc = accuracy\n",
    "            best_nest = nest\n",
    "            best_lr = lr\n",
    "    \n",
    "    print(f'n_estimator = {nest}, learning_rate = {local_lr}, best accuracy = {local_acc} ')\n",
    "\n",
    "print(f'Result : best n_estimator = {best_nest}, best learning_rate = {best_lr}, best accuracy = {best_acc} ')\n",
    "print('---------------------------------------------------------------')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "param = {}\n",
    "\n",
    "param['n_estimators'] = best_nest # after tunning\n",
    "param['max_depth'] = 2\n",
    "param['learning_rate'] = best_lr # after tunning\n",
    "\n",
    "xgb = XGBClassifier(**param)\n",
    "xgb_model = xgb.fit(X_re,y_re)\n",
    "\n",
    "Y_pred = xgb_model.predict(X_test)\n",
    "predictions = [round(value) for value in Y_pred]\n",
    "\n",
    "print('Result for XGboost after grid search (remove_MBTI = False, Model = XGboost, data = SMOTE)')\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "precision_mi = precision_score(y_test, predictions, average='micro')\n",
    "print(\"Micro Precision: %.2f%%\" % (precision_mi * 100.0))\n",
    "precision_ma = precision_score(y_test, predictions, average='macro')\n",
    "print(\"Macro Precision: %.2f%%\" % (precision_ma * 100.0))\n",
    "\n",
    "recall_mi = recall_score(y_test, predictions, average='micro')\n",
    "print(\"Micro Recall: %.2f%%\" % (recall_mi * 100.0))\n",
    "recall_ma = recall_score(y_test, predictions, average='macro')\n",
    "print(\"Macro Recall: %.2f%%\" % (recall_ma * 100.0))\n",
    "\n",
    "f1score_mi = f1_score(y_test, predictions, average='micro')\n",
    "print(\"Micro F1score: %.2f%%\" % (f1score_mi * 100.0))\n",
    "f1score_ma = f1_score(y_test, predictions, average='macro')\n",
    "print(\"Macro F1score: %.2f%%\" % (f1score_ma * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 : 4 models (remove_MBTI = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>IE</th>\n",
       "      <th>NS</th>\n",
       "      <th>TF</th>\n",
       "      <th>JP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8670</th>\n",
       "      <td>ISFP</td>\n",
       "      <td>'https://www.youtube.com/watch?v=t8edHB_h908||...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8671</th>\n",
       "      <td>ENFP</td>\n",
       "      <td>'So...if this thread already exists someplace ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8672</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'So many questions when i do these things.  I ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8673</th>\n",
       "      <td>INFP</td>\n",
       "      <td>'I am very conflicted right now when it comes ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8674</th>\n",
       "      <td>INFP</td>\n",
       "      <td>'It has been too long since I have been on per...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8675 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      type                                              posts  IE  NS  TF  JP\n",
       "0     INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...   1   1   0   1\n",
       "1     ENTP  'I'm finding the lack of me in these posts ver...   0   1   1   0\n",
       "2     INTP  'Good one  _____   https://www.youtube.com/wat...   1   1   1   0\n",
       "3     INTJ  'Dear INTP,   I enjoyed our conversation the o...   1   1   1   1\n",
       "4     ENTJ  'You're fired.|||That's another silly misconce...   0   1   1   1\n",
       "...    ...                                                ...  ..  ..  ..  ..\n",
       "8670  ISFP  'https://www.youtube.com/watch?v=t8edHB_h908||...   1   0   0   0\n",
       "8671  ENFP  'So...if this thread already exists someplace ...   0   1   0   0\n",
       "8672  INTP  'So many questions when i do these things.  I ...   1   1   1   0\n",
       "8673  INFP  'I am very conflicted right now when it comes ...   1   1   0   0\n",
       "8674  INFP  'It has been too long since I have been on per...   1   1   0   0\n",
       "\n",
       "[8675 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../data/Kaggle_MBTI.csv')\n",
    "def get_types(row):\n",
    "    t=row['type']\n",
    "\n",
    "    I = 0; N = 0\n",
    "    T = 0; J = 0\n",
    "    \n",
    "    if t[0] == 'I': I = 1\n",
    "    elif t[0] == 'E': I = 0\n",
    "    else: print('I-E not found') \n",
    "        \n",
    "    if t[1] == 'N': N = 1\n",
    "    elif t[1] == 'S': N = 0\n",
    "    else: print('N-S not found')\n",
    "        \n",
    "    if t[2] == 'T': T = 1\n",
    "    elif t[2] == 'F': T = 0\n",
    "    else: print('T-F not found')\n",
    "        \n",
    "    if t[3] == 'J': J = 1\n",
    "    elif t[3] == 'P': J = 0\n",
    "    else: print('J-P not found')\n",
    "    return pd.Series( {'IE':I, 'NS':N , 'TF': T, 'JP': J }) \n",
    "\n",
    "data = data.join(data.apply (lambda row: get_types (row),axis=1))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introversion (I) /  Extroversion (E):\t 1999  /  6676\n",
      "Intuition (N) / Sensing (S):\t\t 1197  /  7478\n",
      "Thinking (T) / Feeling (F):\t\t 4694  /  3981\n",
      "Judging (J) / Perceiving (P):\t\t 5241  /  3434\n"
     ]
    }
   ],
   "source": [
    "print (\"Introversion (I) /  Extroversion (E):\\t\", data['IE'].value_counts()[0], \" / \", data['IE'].value_counts()[1])\n",
    "print (\"Intuition (N) / Sensing (S):\\t\\t\", data['NS'].value_counts()[0], \" / \", data['NS'].value_counts()[1])\n",
    "print (\"Thinking (T) / Feeling (F):\\t\\t\", data['TF'].value_counts()[0], \" / \", data['TF'].value_counts()[1])\n",
    "print (\"Judging (J) / Perceiving (P):\\t\\t\", data['JP'].value_counts()[0], \" / \", data['JP'].value_counts()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binarize MBTI list: \n",
      "[[0 0 0 0]\n",
      " [1 0 1 1]\n",
      " [0 0 1 1]\n",
      " ...\n",
      " [0 0 1 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Splitting the MBTI personality into 4 letters and binarizing it\n",
    "\n",
    "b_Pers = {'I':0, 'E':1, 'N':0, 'S':1, 'F':0, 'T':1, 'J':0, 'P':1}\n",
    "b_Pers_list = [{0:'I', 1:'E'}, {0:'N', 1:'S'}, {0:'F', 1:'T'}, {0:'J', 1:'P'}]\n",
    "\n",
    "def translate_personality(personality):\n",
    "    # transform mbti to binary vector\n",
    "    return [b_Pers[l] for l in personality]\n",
    "\n",
    "#To show result output for personality prediction\n",
    "def translate_back(personality):\n",
    "    # transform binary vector to mbti personality\n",
    "    s = \"\"\n",
    "    for i, l in enumerate(personality):\n",
    "        s += b_Pers_list[i][l]\n",
    "    return s\n",
    "\n",
    "list_personality_bin = np.array([translate_personality(p) for p in data.type])\n",
    "print(\"Binarize MBTI list: \\n%s\" % list_personality_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8675, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_personality=[]\n",
    "for row in data.iterrows():\n",
    "    type_labelized = translate_personality(row[1].type) #or use lab_encoder.transform([row[1].type])[0]\n",
    "    list_personality.append(type_labelized)\n",
    "list_personality = np.array(list_personality)\n",
    "list_personality.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IE: Introversion (I) / Extroversion (E)\n",
      "NS: Intuition (N) / Sensing (S)\n",
      "FT: Feeling (F) / Thinking (T)\n",
      "JP: Judging (J) / Perceiving (P)\n"
     ]
    }
   ],
   "source": [
    "personality_type = [ \"IE: Introversion (I) / Extroversion (E)\", \"NS: Intuition (N) / Sensing (S)\", \n",
    "                   \"FT: Feeling (F) / Thinking (T)\", \"JP: Judging (J) / Perceiving (P)\"  ]\n",
    "\n",
    "for l in range(len(personality_type)):\n",
    "    print(personality_type[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8675, 77959)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data = raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=100)\n",
    "X = svd.fit_transform(X_tfidf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IE: Introversion (I) / Extroversion (E) Accuracy: 77.65%\n",
      "NS: Intuition (N) / Sensing (S) Accuracy: 85.47%\n",
      "FT: Feeling (F) / Thinking (T) Accuracy: 75.69%\n",
      "JP: Judging (J) / Perceiving (P) Accuracy: 65.91%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# XGBoost Model create\n",
    "for l in range(len(personality_type)):\n",
    "    \n",
    "    Y = list_personality[:,l]\n",
    "\n",
    "    # split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "\n",
    "    # fit model on training data\n",
    "    param = {}\n",
    "\n",
    "    param['n_estimators'] = 700 # after tunning\n",
    "    param['max_depth'] = 2\n",
    "    param['learning_rate'] = 0.12 # after tunning\n",
    "\n",
    "    model = XGBClassifier(**param)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions for test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    \n",
    "    print(\"%s Accuracy: %.2f%%\" % (personality_type[l], accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data = SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IE: Introversion (I) / Extroversion (E) Accuracy: 72.20%\n",
      "NS: Intuition (N) / Sensing (S) Accuracy: 79.88%\n",
      "FT: Feeling (F) / Thinking (T) Accuracy: 75.83%\n",
      "JP: Judging (J) / Perceiving (P) Accuracy: 64.41%\n"
     ]
    }
   ],
   "source": [
    "# XGBoost Model create\n",
    "for l in range(len(personality_type)):\n",
    "    \n",
    "    Y = list_personality[:,l]\n",
    "\n",
    "    # split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "\n",
    "    # SMOTE\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    X_re, y_re = SMOTE(random_state=42).fit_resample(X_train, y_train)\n",
    "\n",
    "    # fit model on training data\n",
    "    param = {}\n",
    "\n",
    "    param['n_estimators'] = 1000 # after tunning\n",
    "    param['max_depth'] = 2\n",
    "    param['learning_rate'] = 0.2 # after tunning\n",
    "\n",
    "    model = XGBClassifier(**param)\n",
    "    model.fit(X_re, y_re)\n",
    "\n",
    "    # make predictions for test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    \n",
    "    print(\"%s Accuracy: %.2f%%\" % (personality_type[l], accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x = [1,2,3,4]\n",
    "y = [1,2,2,4]\n",
    "accuracy = accuracy_score(x, y)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 : Combine 4 models (remove_MBTI = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### remove_MBTI = True, data = raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IE: Introversion (I) / Extroversion (E)\n",
      "[0 0 0 ... 0 0 0]\n",
      "NS: Intuition (N) / Sensing (S)\n",
      "[0 0 0 ... 0 0 0]\n",
      "FT: Feeling (F) / Thinking (T)\n",
      "[1 1 1 ... 0 0 0]\n",
      "JP: Judging (J) / Perceiving (P)\n",
      "[1 1 1 ... 0 1 1]\n",
      "The final Accuracy is : 0.34299685644428923\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, list_personality, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "final_pred = np.zeros((X_test.shape[0], 4)) # 紀錄 4 個 model 分別預測病combine後的 MBTI 結果\n",
    "\n",
    "# XGBoost Model create\n",
    "for l in range(len(personality_type)):\n",
    "    \n",
    "    Y = y_train[:,l]\n",
    "\n",
    "\n",
    "    # fit model on training data\n",
    "    param = {}\n",
    "\n",
    "    param['n_estimators'] = 700 # after tunning\n",
    "    param['max_depth'] = 2\n",
    "    param['learning_rate'] = 0.12 # after tunning\n",
    "\n",
    "    model = XGBClassifier(**param)\n",
    "    model.fit(X_train, y_train[:,l])\n",
    "\n",
    "    # make predictions for test data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # 將四個預測結果合併成最後 MBTI 結果\n",
    "    for j in range(len(y_pred)):\n",
    "        final_pred[j][l] = y_pred[j]\n",
    "\n",
    "\n",
    "    # predictions = [round(value) for value in y_pred]\n",
    "    # # evaluate predictions\n",
    "    # accuracy = accuracy_score(y_test, predictions)\n",
    "    \n",
    "    print(personality_type[l])\n",
    "    print(y_pred)\n",
    "\n",
    "accuracy = accuracy_score(y_test, final_pred)\n",
    "print(f'The final Accuracy is : {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### remove_MBTI = True, data = SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IE: Introversion (I) / Extroversion (E)\n",
      "[0 0 0 ... 0 1 0]\n",
      "NS: Intuition (N) / Sensing (S)\n",
      "[0 0 0 ... 0 0 0]\n",
      "FT: Feeling (F) / Thinking (T)\n",
      "[1 1 1 ... 0 0 0]\n",
      "JP: Judging (J) / Perceiving (P)\n",
      "[1 1 1 ... 0 0 1]\n",
      "The final Accuracy is : 0.2888578414250786\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, list_personality, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "final_pred = np.zeros((X_test.shape[0], 4)) # 紀錄 4 個 model 分別預測病combine後的 MBTI 結果\n",
    "\n",
    "# XGBoost Model create\n",
    "for l in range(len(personality_type)):\n",
    "    \n",
    "    Y = y_train[:,l]\n",
    "\n",
    "\n",
    "    # SMOTE\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    X_re, y_re = SMOTE(random_state=42).fit_resample(X_train, Y)\n",
    "    # fit model on training data\n",
    "    param = {}\n",
    "\n",
    "    param['n_estimators'] = 1000 # after tunning\n",
    "    param['max_depth'] = 2\n",
    "    param['learning_rate'] = 0.2 # after tunning\n",
    "\n",
    "    model = XGBClassifier(**param)\n",
    "    model.fit(X_re, y_re)\n",
    "\n",
    "    # make predictions for test data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # 將四個預測結果合併成最後 MBTI 結果\n",
    "    for j in range(len(y_pred)):\n",
    "        final_pred[j][l] = y_pred[j]\n",
    "\n",
    "\n",
    "    # predictions = [round(value) for value in y_pred]\n",
    "    # # evaluate predictions\n",
    "    # accuracy = accuracy_score(y_test, predictions)\n",
    "    \n",
    "    print(personality_type[l])\n",
    "    print(y_pred)\n",
    "\n",
    "accuracy = accuracy_score(y_test, final_pred)\n",
    "print(f'The final Accuracy is : {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4a4ba1db5610b5ef8ea6f761f191f567ff5b409f02c1cd41f72dbaaf0303b4b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
